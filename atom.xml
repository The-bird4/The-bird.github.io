<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2019-03-18T13:55:17.000Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</id>
    <published>2019-03-12T16:00:00.000Z</published>
    <updated>2019-03-18T13:55:17.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fz1ts9827yj30zk0kj46e.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题：</p><blockquote><p>通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？</p></blockquote><p>这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。</p><p>跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？</p><h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><p>首先想到的是这个肯定可以在配置文件中有设置的地方的：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g123nirw3ij31f6078tbw.jpg" alt=""></p><h3 id="谷歌大法好"><a href="#谷歌大法好" class="headerlink" title="谷歌大法好"></a>谷歌大法好</h3><p>虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g123p8492mj30w20c6mzh.jpg" alt=""></p><p>找到了一条，点进去看 Issue 如下：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g123qvoustj31an0u07cg.jpg" alt=""></p><p>发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g12410547aj31mr0u07m0.jpg" alt=""></p><h3 id="源码确认"><a href="#源码确认" class="headerlink" title="源码确认"></a>源码确认</h3><p>这个 <code>jobmanager.web.upload.dir</code> 是不是？我去看下 1.8 的源码确认一下：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1245hxiunj317z0u0k26.jpg" alt=""></p><p>发现这个 <code>jobmanager.web.upload.dir</code> 还过期了，用 <code>WebOptions</code> 类中的 <code>UPLOAD_DIR</code> 替代了！</p><p>继续跟进去看看这个 <code>UPLOAD_DIR</code> 是啥玩意？</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1249b9477j31v40imn34.jpg" alt=""></p><p>看这注释的意思是说，如果这个配置 <code>web.upload.dir</code> 没有配置具体的路径的话就会使用 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 目录，那么我们来看看是否配置了这个目录呢？</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g124bxp11sj31i406wn0n.jpg" alt=""></p><p>确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 是在哪里的？</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g124fjs73gj31ba0u0th0.jpg" alt=""></p><p>又是一个过期的目录，mmp，继续跟下去看下这个目录 <code>TMP_DIR</code>。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g124i6o2rbj31f80ewgpr.jpg" alt=""></p><p>我们查看下配置文件是否有配置这个 <code>web.tmpdir</code> 的值，又是没有：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g124j30yo9j31i609ajwi.jpg" alt=""></p><p>so，它肯定使用的是 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 这个目录了，</p><p>我查看了下我本地电脑起的 job 它的配置中有这个配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g124ovo6r7j31a00aa0u4.jpg" alt=""></p><p>再观察了下 job，发现 jobManager 这里有个 <code>web.tmpdir</code> 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g124rm2kgej31im0u0dlq.jpg" alt=""></p><p>发现这个 <code>web.tmpdir</code> 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！</p><h3 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h3><p>进入这个目录发现我们上传的 jar 终于被找到了：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g124xn6fkij32640gqdpx.jpg" alt=""></p><h3 id="配置上传-jar-目录确认"><a href="#配置上传-jar-目录确认" class="headerlink" title="配置上传 jar 目录确认"></a>配置上传 jar 目录确认</h3><p>上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1252bhsdtj31340io7b7.jpg" alt=""></p><p>更改之后再看 <code>web.tmpdir</code> 是这样的:</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g125c44rmnj31u10u0dkx.jpg" alt=""></p><p>从 Flink UI 上上传了三个 jar，查看 <code>/usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179</code> 目录下就出现了我们的 jar 了。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g125hf804lj32300h0tin.jpg" alt=""></p><p>我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.upload.dir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p>这样的话，就可以保证你的 jar 不再会被删除了！</p><p>再来看看源码是咋写的哈：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从配置文件中找 UPLOAD_DIR</span></span><br><span class="line"><span class="keyword">final</span> Path uploadDir = Paths.get(</span><br><span class="line">config.getString(WebOptions.UPLOAD_DIR,config.getString(WebOptions.TMP_DIR)),</span><br><span class="line"><span class="string">"flink-web-upload"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RestServerEndpointConfiguration(</span><br><span class="line">restAddress,restBindAddress,port,sslEngineFactory,</span><br><span class="line">uploadDir,maxContentLength,responseHeaders);</span><br></pre></td></tr></table></figure><p>他就是从配置文件中找 <code>UPLOAD_DIR</code>，如果为 <code>null</code> 就找 <code>TMP_DIR</code> 目录来当作 jar 上传的路径！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fz1ts9827yj30zk0kj46e.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴开源的 Blink 实时计算框架真香</title>
    <link href="http://www.54tianzhisheng.cn/2019/02/28/blink/"/>
    <id>http://www.54tianzhisheng.cn/2019/02/28/blink/</id>
    <published>2019-02-27T16:00:00.000Z</published>
    <updated>2019-03-03T03:28:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fy7f3ylm7hj30zk0pf11z.jpg" alt=""></p><a id="more"></a><p>Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？</p><p>我们先看看 Blink 黑色版本：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0mcxr7akrj31c00u0n7l.jpg" alt=""></p><p>对比下 Flink 版本你就知道黑色版本多好看了。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0mebx2ovoj31jl0u0wq7.jpg" alt=""></p><p>你上传 jar 包的时候是这样的：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0mess15oej31c00u0q8e.jpg" alt=""></p><p>我们来看看 Blink 运行的 job 长啥样？</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0md5lozakj31c00u07f4.jpg" alt=""></p><p>再来对比一下 Flink 的样子：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0meejgw9pj31hx0u0k0m.jpg" alt=""></p><p>查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0mdy4qkemj31c00u0am0.jpg" alt=""></p><p>查看 subTask，这里可以直接点击这个日志就可以查看 task 日志：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0mdzkffgxj31c00u0gx0.jpg" alt=""></p><p>查看背压：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0me0y0a23j31c00u013l.jpg" alt=""></p><p>查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0mdwetq1dj31c00u0qh6.jpg" alt=""></p><p>查看 job 运行时间段的情况：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0me4s2dfcj31c00u0nc5.jpg" alt=""></p><p>查看 running 的 job：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0mcw0ytn6j31c00u0wk6.jpg" alt=""></p><p>查看已经完成的 job：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0mcx0we4fj31c00u07ag.jpg" alt=""></p><p>查看 Task Manager：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0mcu9wkzlj31c00u0wk5.jpg" alt=""></p><p>Task Manager 分配的资源详情：</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0mctwhf2sj31c00u0n4d.jpg" alt=""></p><p>Task Manager metric 监控信息详情：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0mctn5nasj31c00u0qax.jpg" alt=""></p><p>Task Manager log 文件详情，包含运行产生的日志和 GC 日志：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0mct51v0mj31c00u0wpf.jpg" alt=""></p><p>Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0mcskyxlyj31c00u0kdj.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">http://www.54tianzhisheng.cn/2019/02/28/blink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fy7f3ylm7hj30zk0pf11z.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Blink" scheme="http://www.54tianzhisheng.cn/tags/Blink/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</id>
    <published>2019-01-19T16:00:00.000Z</published>
    <updated>2019-03-17T13:46:49.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fz1todvwunj30zk0k0qbv.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前有文章 <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a> 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。</p><h3 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h3><h4 id="安装-RabbitMQ"><a href="#安装-RabbitMQ" class="headerlink" title="安装 RabbitMQ"></a>安装 RabbitMQ</h4><p>这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。</p><p>在命令行中执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d  -p 15672:15672  -p  5672:5672  -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0r72iagqtj327w0sowqh.jpg" alt=""></p><p>对这个命令不懂的童鞋可以看看我以前的文章：<a href="http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/">http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/</a></p><p>登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0r7bx9wgej31c00u0dqw.jpg" alt=""></p><h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><p>pom.xml 中添加 Flink connector rabbitmq 的依赖如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-rabbitmq_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。</p><p>注意按照我的步骤来一步步操作，否则可能会出现一些错误！</p><p>RabbitMQProducerUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Channel;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Connection;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.ConnectionFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RabbitMQProducerUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建连接工厂</span></span><br><span class="line">        ConnectionFactory factory = <span class="keyword">new</span> ConnectionFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置RabbitMQ相关信息</span></span><br><span class="line">        factory.setHost(<span class="string">"localhost"</span>);</span><br><span class="line">        factory.setUsername(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPassword(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPort(<span class="number">5672</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个新的连接</span></span><br><span class="line">        Connection connection = factory.newConnection();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个通道</span></span><br><span class="line">        Channel channel = connection.createChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//发送消息到队列中</span></span><br><span class="line">        String message = <span class="string">"Hello zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//我们这里演示发送一千条数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            channel.basicPublish(<span class="string">""</span>, QUEUE_NAME, <span class="keyword">null</span>, (message + i).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            System.out.println(<span class="string">"Producer Send +'"</span> + message + i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭通道和连接</span></span><br><span class="line">        channel.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-主程序"><a href="#Flink-主程序" class="headerlink" title="Flink 主程序"></a>Flink 主程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.ExecutionEnvUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.RMQSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从 rabbitmq 读取数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值</span></span><br><span class="line">        <span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(<span class="string">"localhost"</span>).setVirtualHost(<span class="string">"/"</span>)</span><br><span class="line">                .setPort(<span class="number">5672</span>).setUserName(<span class="string">"admin"</span>).setPassword(<span class="string">"admin"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; zhisheng = env.addSource(<span class="keyword">new</span> RMQSource&lt;&gt;(connectionConfig,</span><br><span class="line">                <span class="string">"zhisheng"</span>,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line">        zhisheng.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(10000);</span></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors rabbitmq"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 RabbitMQProducerUtil 类，再运行 Main 类！</p><p><strong>注意</strong>⚠️：</p><p>1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.</span><br></pre></td></tr></table></figure><p>不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0r7p2ljhqj31bj0u0dub.jpg" alt=""></p><p>2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br></pre></td></tr></table></figure><p>就会出现这种错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10)</span><br></pre></td></tr></table></figure><p>这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 <figure class="highlight plain"><figcaption><span>的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 ```zhisheng``` 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证：</span><br><span class="line"></span><br><span class="line">+ Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。</span><br><span class="line">+ At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了</span><br><span class="line">+ No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ</span><br><span class="line"></span><br><span class="line">### Sink 数据到 RabbitMQ</span><br><span class="line"></span><br><span class="line">RabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public class Main1 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        final RMQConnectionConfig connectionConfig = new RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(&quot;localhost&quot;).setVirtualHost(&quot;/&quot;)</span><br><span class="line">                .setPort(5672).setUserName(&quot;admin&quot;).setPassword(&quot;admin&quot;)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        //注意，换一个新的 queue，否则也会报错</span><br><span class="line">        data.addSink(new RMQSink&lt;&gt;(connectionConfig, &quot;zhisheng001&quot;, new MetricSchema()));</span><br><span class="line">        env.execute(&quot;flink learning connectors rabbitmq&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。</p><p>不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0v656n97qj31nk0tqjwz.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fz1todvwunj30zk0k0qbv.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
      <category term="RabbitMQ" scheme="http://www.54tianzhisheng.cn/tags/RabbitMQ/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</id>
    <published>2019-01-14T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy7f3o7o1uj30zk0npgrq.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前其实在 <a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a> 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0gmlwx69dj310s0e6abt.jpg" alt=""></p><p>于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>你需要将这两个依赖添加到 pom.xml 中</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="读取-kafka-数据"><a href="#读取-kafka-数据" class="headerlink" title="读取 kafka 数据"></a>读取 kafka 数据</h3><p>这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.GsonUtil;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 往kafka中写数据,可以使用这个main函数进行测试</span></span><br><span class="line"><span class="comment"> * Created by zhisheng on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(student));</span><br><span class="line">            Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>); <span class="comment">//发送一条数据 sleep 10s，相当于 1 分钟 6 条</span></span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 kafka 中读取数据，然后序列化成 student 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">        <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">        props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">        .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//，解析字符串成 student 对象</span></span><br></pre></td></tr></table></figure><p>因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">        <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">            out.collect(students);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="写入数据库"><a href="#写入数据库" class="headerlink" title="写入数据库"></a>写入数据库</h3><p>这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-dbcp2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你想使用其他的数据库连接池请加入对应的依赖。</p><p>这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.sinks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbcp2.BasicDataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.sql.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 数据批量 sink 数据到 mysql</span></span><br><span class="line"><span class="comment"> * Created by zhisheng_tian on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">List</span>&lt;<span class="title">Student</span>&gt;&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    BasicDataSource dataSource;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        dataSource = <span class="keyword">new</span> BasicDataSource();</span><br><span class="line">        connection = getConnection(dataSource);</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(List&lt;Student&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//遍历数据集合</span></span><br><span class="line">        <span class="keyword">for</span> (Student student : value) &#123;</span><br><span class="line">            ps.setInt(<span class="number">1</span>, student.getId());</span><br><span class="line">            ps.setString(<span class="number">2</span>, student.getName());</span><br><span class="line">            ps.setString(<span class="number">3</span>, student.getPassword());</span><br><span class="line">            ps.setInt(<span class="number">4</span>, student.getAge());</span><br><span class="line">            ps.addBatch();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[] count = ps.executeBatch();<span class="comment">//批量后执行</span></span><br><span class="line">        System.out.println(<span class="string">"成功了插入了"</span> + count.length + <span class="string">"行数据"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">(BasicDataSource dataSource)</span> </span>&#123;</span><br><span class="line">        dataSource.setDriverClassName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        <span class="comment">//注意，替换成自己本地的 mysql 数据库地址和用户名、密码</span></span><br><span class="line">        dataSource.setUrl(<span class="string">"jdbc:mysql://localhost:3306/test"</span>);</span><br><span class="line">        dataSource.setUsername(<span class="string">"root"</span>);</span><br><span class="line">        dataSource.setPassword(<span class="string">"root123456"</span>);</span><br><span class="line">        <span class="comment">//设置连接池的一些参数</span></span><br><span class="line">        dataSource.setInitialSize(<span class="number">10</span>);</span><br><span class="line">        dataSource.setMaxTotal(<span class="number">50</span>);</span><br><span class="line">        dataSource.setMinIdle(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            con = dataSource.getConnection();</span><br><span class="line">            System.out.println(<span class="string">"创建连接池："</span> + con);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="核心类-Main"><a href="#核心类-Main" class="headerlink" title="核心类 Main"></a>核心类 Main</h3><p>核心程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//</span></span><br><span class="line">        student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">                <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">                    out.collect(students);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> SinkToMySQL());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h3><p>运行 Main 类后再运行 KafkaUtils.java 类！</p><p>下图是往 Kafka 中发送的数据：</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0gtv373k8j31an0u0nid.jpg" alt=""></p><p>下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0gtzx8htbj31c00u0qno.jpg" alt=""></p><p>下图是批量插入数据库的结果：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0gu2g90btj30u012k4bw.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fy7f3o7o1uj30zk0npgrq.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="MySQL" scheme="http://www.54tianzhisheng.cn/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</id>
    <published>2019-01-13T16:00:00.000Z</published>
    <updated>2019-03-17T14:40:15.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fy7f176d0sj30zk0npwio.jpg" alt=""><br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之所以写这个是因为前段时间自己的项目出现过这样的一个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzd2l2niwyj31nd0u0qod.jpg" alt=""></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a><br>，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzd2yv9fj7j31la0u0wqd.jpg" alt=""></p><p>竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。</p><h3 id="什么是-parallelism？"><a href="#什么是-parallelism？" class="headerlink" title="什么是 parallelism？"></a>什么是 parallelism？</h3><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzd4t1vicoj30w20ju77a.jpg" alt=""></p><p>如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-parallelism？"><a href="#如何设置-parallelism？" class="headerlink" title="如何设置 parallelism？"></a>如何设置 parallelism？</h3><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzd4yrlh50j31v00og156.jpg" alt=""></p><p>如上图，在 flink 配置文件中可以查看到默认并行度是 1，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以你如何在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。</p><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过这样来设置你整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><p>如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><p>并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？</p><h3 id="什么是-slot？"><a href="#什么是-slot？" class="headerlink" title="什么是 slot？"></a>什么是 slot？</h3><p>其实什么是 slot 这个问题之前在第一篇文章 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 中就介绍过了，这里再讲细一点。</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fw7ovrkditj30op0fpjsp.jpg" alt=""></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p>文字说的比较干，zhisheng 这里我就拿下面的图片来讲解：</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw96irwzj5j30pj0c3dir.jpg" alt=""></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。</p><p>但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 </p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzd5sf0f1sj31ts0bq79w.jpg" alt=""></p><h3 id="slot-和-parallelism"><a href="#slot-和-parallelism" class="headerlink" title="slot 和 parallelism"></a>slot 和 parallelism</h3><p>下面给出官方的图片来更加深刻的理解下 slot：</p><p>1、slot 是指 taskmanager 的并发执行能力</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzd5pllxhej30ww0cdgoi.jpg" alt=""></p><p>taskmanager.numberOfTaskSlots:3</p><p>每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。</p><p>2、parallelism 是指 taskmanager 实际使用的并发能力</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzd60r711jj30wy09pjtv.jpg" alt=""></p><p>parallelism.default:1 </p><p>运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzd61wswyuj30x60jj0zg.jpg" alt=""></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzd66pvuvzj30xa09qgp4.jpg" alt=""></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><p>好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。</p><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。</p><p>再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.setParallelism(15);</span><br></pre></td></tr></table></figure><p>为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fy7f176d0sj30zk0npwio.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink JobManager 高可用性配置</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</id>
    <published>2019-01-12T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy7f0wruagj30zk0k0q67.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前在 <a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a> 讲过 Flink 的配置，但是后面陆续有人来问我一些配置相关的东西，在加上我现在对 Flink 也更熟悉了些，这里我就再写下 Flink JobManager 的配置相关信息。</p><p>在 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 一文中介绍过了 Flink Job 的运行架构图：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fw7ovrkditj30op0fpjsp.jpg" alt=""></p><p>JobManager 协调每个 Flink 作业的部署。它负责调度和资源管理。</p><p>默认情况下，每个 Flink 集群都有一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。</p><p>如果我们使用 JobManager 高可用模式，可以避免这个问题。您可以为 standalone 集群和 YARN 集群配置高可用模式。</p><h2 id="standalone-集群高可用性"><a href="#standalone-集群高可用性" class="headerlink" title="standalone 集群高可用性"></a>standalone 集群高可用性</h2><p>standalone 集群的 JobManager 高可用性的概念是，任何时候都有一个主 JobManager 和 多个备 JobManagers，以便在主节点失败时有新的 JobNamager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别。每个 JobManager 都可以充当主备节点。</p><p>例如，请考虑以下三个 JobManager 实例的设置：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fz9x5tv317j31540ns0w4.jpg" alt=""></p><h3 id="如何配置"><a href="#如何配置" class="headerlink" title="如何配置"></a>如何配置</h3><p>要启用 JobManager 高可用性功能，您必须将高可用性模式设置为 zookeeper，配置 ZooKeeper quorum，将所有 JobManagers 主机及其 Web UI 端口写入配置文件。</p><p>Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。<br>他在我们的 Flink 安装路径下面 /conf/zoo.cfg 。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fz9xbm2gf4j31110u0k8o.jpg" alt=""></p><h3 id="Masters-文件"><a href="#Masters-文件" class="headerlink" title="Masters 文件"></a>Masters 文件</h3><p>要启动 HA 集群，请在以下位置配置 Masters 文件 conf/masters：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">xxx.xxx.xxx.xxx:8081</span><br></pre></td></tr></table></figure><p>masters 文件包含启动 JobManagers 的所有主机以及 Web 用户界面绑定的端口，上面一行写一个。</p><p>默认情况下，job manager 选一个随机端口作为进程通信端口。您可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。</p><h3 id="配置文件-flink-conf-yaml"><a href="#配置文件-flink-conf-yaml" class="headerlink" title="配置文件 (flink-conf.yaml)"></a>配置文件 (flink-conf.yaml)</h3><p>要启动 HA 集群，请将以下配置键添加到 conf/flink-conf.yaml：</p><p>高可用性模式（必需）：在 conf/flink-conf.yaml中，必须将高可用性模式设置为 zookeeper，以打开高可用模式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br></pre></td></tr></table></figure><p>ZooKeeper quorum（必需）：ZooKeeper quorum 是一组 ZooKeeper 服务器，它提供分布式协调服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181</span><br></pre></td></tr></table></figure><p>每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 zookeeper。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fz9xo0vy7zj31hz0u04ar.jpg" alt=""></p><p>另外就是高可用存储目录，JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS, S3, Ceph, nfs 等，该 storageDir 中保存了 JobManager 恢复状态需要的所有元数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.storageDir: hdfs:///flink/ha/</span><br></pre></td></tr></table></figure><p>配置 master 文件和 ZooKeeper 配置后，您可以使用提供的集群启动脚本。他们将启动 HA 集群。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 集群配置单独的 ZooKeeper 根路径。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>具有 2 个 JobManagers 的 Standalone 集群：</p><p>1、在 conf/flink-conf.yaml 中配置高可用模式和 Zookeeper :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/recovery</span><br></pre></td></tr></table></figure><p>2、在 conf/masters 中 配置 masters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">localhost:8082</span><br></pre></td></tr></table></figure><p>3、在 conf/zoo.cfg 中配置 Zookeeper 服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure><p>4、启动 ZooKeeper 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure><p>5、启动一个 Flink HA 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-cluster.sh</span><br><span class="line">Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting taskmanager daemon on host localhost.</span><br></pre></td></tr></table></figure><p>6、停止 ZooKeeper 和集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/stop-cluster.sh</span><br><span class="line">Stopping taskmanager daemon (pid: 7647) on localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7495) on host localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7349) on host localhost.</span><br><span class="line"></span><br><span class="line">$ bin/stop-zookeeper-quorum.sh</span><br><span class="line">Stopping zookeeper daemon (pid: 7101) on host localhost.</span><br></pre></td></tr></table></figure><p>上面的执行脚本如下图可见：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fz9y3uemrjj31js0u0twx.jpg" alt=""></p><h2 id="YARN-集群高可用性"><a href="#YARN-集群高可用性" class="headerlink" title="YARN 集群高可用性"></a>YARN 集群高可用性</h2><p>当运行高可用的 YARN 集群时，我们不会运行多个 JobManager 实例，而只会运行一个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于您使用的 YARN 版本。</p><h3 id="如何配置？"><a href="#如何配置？" class="headerlink" title="如何配置？"></a>如何配置？</h3><h3 id="Application-Master-最大重试次数-yarn-site-xml"><a href="#Application-Master-最大重试次数-yarn-site-xml" class="headerlink" title="Application Master 最大重试次数 (yarn-site.xml)"></a>Application Master 最大重试次数 (yarn-site.xml)</h3><p>在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。</p><h3 id="Application-Attempts-flink-conf-yaml"><a href="#Application-Attempts-flink-conf-yaml" class="headerlink" title="Application Attempts (flink-conf.yaml)"></a>Application Attempts (flink-conf.yaml)</h3><p>除了上面可以配置最大重试次数外，你还可以在 flink-conf.yaml 配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><p>这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，yarn 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。</p><h3 id="容器关闭行为"><a href="#容器关闭行为" class="headerlink" title="容器关闭行为"></a>容器关闭行为</h3><ul><li>YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。</li><li>YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 task manager 启动时申请资源的时间。</li><li>YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序。这避免了持久的工作会耗尽它的应用程序尝试。</li></ul><h3 id="示例：高可用的-YARN-Session"><a href="#示例：高可用的-YARN-Session" class="headerlink" title="示例：高可用的 YARN Session"></a>示例：高可用的 YARN Session</h3><p>1、配置 HA 模式和 Zookeeper 集群 在 conf/flink-conf.yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure></p><p>2、配置 ZooKeeper 服务 在 conf/zoo.cfg：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure></p><p>3、启动 Zookeeper 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure></p><p>4、启动 HA 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/yarn-session.sh -n 2</span><br></pre></td></tr></table></figure></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章再次写了下 Flink JobManager 的高可用配置，如何在 standalone 集群和 YARN 集群中配置高可用。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fy7f0wruagj30zk0k0q67.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 写入数据到 Kafka</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</id>
    <published>2019-01-05T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy7f0n3yyhj30zk0n2jwu.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 写了如何将 Kafka 中的数据存储到 ElasticSearch 中，里面其实就已经用到了 Flink 自带的 Kafka source connector（FlinkKafkaConsumer）。存入到 ES 只是其中一种情况，那么如果我们有多个地方需要这份通过 Flink 转换后的数据，是不是又要我们继续写个 sink 的插件呢？确实，所以 Flink 里面就默认支持了不少 sink，比如也支持 Kafka sink connector（FlinkKafkaProducer），那么这篇文章我们就讲讲如何将数据写入到 Kafka。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fyx7h03d6uj319u0p240r.jpg" alt=""></p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>Flink 里面支持 Kafka 0.8、0.9、0.10、0.11 ，以后有时间可以分析下源码的实现。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fywplry4yfj30xo0u0dne.jpg" alt=""></p><p>这里我们需要安装下 Kafka，请对应添加对应的 Flink Kafka connector 依赖的版本，这里我们使用的是 0.11 版本：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Kafka-安装"><a href="#Kafka-安装" class="headerlink" title="Kafka 安装"></a>Kafka 安装</h4><p>这里就不写这块内容了，可以参考我以前的文章 <a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/">Kafka 安装及快速入门</a>。</p><p>这里我们演示把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fywvs5kzmlj32660qmami.jpg" alt=""></p><p>可以看到本地的 Kafka 是没有任何 topic 的，如果等下我们的程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明我的程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。</p><h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fywvy6y1zcj31i90u0ke6.jpg" alt=""></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fyww276dypj321a0lygyd.jpg" alt=""></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>上面代码我们使用 Flink Kafka Producer 只传了三个参数：brokerList、topicId、serializationSchema（序列化）</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fywwcv7vd4j31o50u0k70.jpg" alt=""></p><p>其实也可以传入多个参数进去，现在有的参数用的是默认参数，因为这个内容比较多，后面可以抽出一篇文章单独来讲。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了 Flink 读取其他 Kafka 集群的数据，然后写入到本地的 Kafka 上。我在 Flink 这层没做什么数据转换，只是原样的将数据转发了下，如果你们有什么其他的需求，是可以在 Flink 这层将数据进行各种转换操作，比如这篇文章中的一些转换：<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a>，然后将转换后的数据发到 Kafka 上去。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNbRwly1fy7f0n3yyhj30zk0n2jwu.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 项目如何运行？</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</id>
    <published>2019-01-04T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fy7f0d4kzdj30zk0jtwis.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前写了不少 Flink 文章了，也有不少 demo，但是文章写的时候都是在本地直接运行 Main 类的 main 方法，其实 Flink 是支持在 UI 上上传 Flink Job 的 jar 包，然后运行得。最开始在第一篇 <a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install/">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a> 中其实提到过了 Flink 自带的 UI 界面，今天我们就来看看如何将我们的项目打包在这里发布运行。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="编译打包"><a href="#编译打包" class="headerlink" title="编译打包"></a>编译打包</h4><p>项目代码就拿我之前的文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 吧，代码地址是在 GitHub 仓库地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a> ，如果感兴趣的可以直接拿来打包试试水。</p><p>我们在整个项目 （flink-learning）pom.xml 所在文件夹执行以下命令打包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fyw0covajbj31c00u0tl1.jpg" alt=""></p><p>然后你会发现在 flink-learning-connectors-es6 的 target 目录下有 flink-learning-connectors-es6-1.0-SNAPSHOT.jar 。</p><h4 id="启动-ES"><a href="#启动-ES" class="headerlink" title="启动 ES"></a>启动 ES</h4><p>注意你的 Kafka 数据源和 ES 都已经启动好了, 清空了下 ES 目录下的 data 数据，为了就是查看是不是真的有数据存入进来了。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fyw0yfu4l4j31j00u0n2x.jpg" alt=""></p><h4 id="提交-jar-包"><a href="#提交-jar-包" class="headerlink" title="提交 jar 包"></a>提交 jar 包</h4><p>将此文件提交到 Flinkserver 上，如下图：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fyw0lxdgw1j313p0u0wm3.jpg" alt=""></p><p>点击下图红框中的”Upload”按钮：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fyw0mvikxwj31k30u0adu.jpg" alt=""></p><p>如下图，选中刚刚上传的文件，填写类名，再点击”Submit”按钮即可启动 Job：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fyw0w7d6e1j31pm0u0dns.jpg" alt=""></p><h3 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h3><p>如下图，在 Overview 页面可见正在运行的任务：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fyw2j7idimj31e60u0aj2.jpg" alt=""></p><p>你可以看到 Task Manager 中关于任务的 metric 数据<br>、日志信息以及 Stdout 信息。</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fyw2nvk6y3j31co0u0hdo.jpg" alt=""></p><p>查看 Kibana ，此时 ES 中已经有数据了：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fyw2oxbc27j31fw0u01bu.jpg" alt=""></p><p>我们可以在 flink ui 界面上的 overview cancel 这个 job，那么可以看到 job 的日志：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fyw3dfaergj31e00u07wh.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fyw3f5alcij31i10u0dp1.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了下如何将我们的 job 编译打包并提交到 Flink 自带到 Server UI 上面去运行，也算是对前面文章的一个补充，当然了，Flink job 不仅支持这种模式的运行，它还可以运行在 K8s，Mesos，等上面，等以后我接触到再写写。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNbRwly1fy7f0d4kzdj30zk0jtwis.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/</id>
    <published>2018-12-29T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy7f02y3nwj30zk0npn22.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前面 FLink 的文章中我们已经介绍了说 Flink 已经有很多自带的 Connector。</p><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>其中包括了 Source 和 Sink 的，后面我也讲了下如何自定义自己的 Source 和 Sink。</p><p>那么今天要做的事情是啥呢？就是介绍一下 Flink 自带的 ElasticSearch Connector，我们今天就用他来做 Sink，将 Kafka 中的数据经过 Flink 处理后然后存储到 ElasticSearch。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>安装 ElasticSearch，这里就忽略，自己找我以前的文章，建议安装 ElasticSearch 6.0 版本以上的，毕竟要跟上时代的节奏。</p><p>下面就讲解一下生产环境中如何使用 Elasticsearch Sink 以及一些注意点，及其内部实现机制。</p><h3 id="Elasticsearch-Sink"><a href="#Elasticsearch-Sink" class="headerlink" title="Elasticsearch Sink"></a>Elasticsearch Sink</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这依赖版本号请自己根据使用的版本对应改变下。</p><p>下面所有的代码都没有把 import 引入到这里来，如果需要查看更详细的代码，请查看我的 GitHub 仓库地址：</p><p><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a></p><p>这个 module 含有本文的所有代码实现，当然越写到后面自己可能会做一些抽象，所以如果有代码改变很正常，请直接查看全部项目代码。</p><h4 id="ElasticSearchSinkUtil-工具类"><a href="#ElasticSearchSinkUtil-工具类" class="headerlink" title="ElasticSearchSinkUtil 工具类"></a>ElasticSearchSinkUtil 工具类</h4><p>这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面文章还会再讲些其他的配置。</p><p>ElasticSearchSinkUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ElasticSearchSinkUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Main-启动类"><a href="#Main-启动类" class="headerlink" title="Main 启动类"></a>Main 启动类</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ElasticSearchSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ElasticSearchSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置都支持集群模式填写，注意用 , 分隔！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>执行 Main 类的 main 方法，我们的程序是只打印 flink 的日志，没有打印存入的日志（因为我们这里没有打日志）：</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fyrfj3fjpbj31c00u01kx.jpg" alt=""></p><p>所以看起来不知道我们的 sink 是否有用，数据是否从 kafka 读取出来后存入到 es 了。</p><p>你可以查看下本地起的 es 终端或者服务器的 es 日志就可以看到效果了。</p><p>es 日志如下：</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fyrfdct4ucj31et0u04qp.jpg" alt=""></p><p>上图是我本地 Mac 电脑终端的 es 日志，可以看到我们的索引了。</p><p>如果还不放心，你也可以在你的电脑装个 kibana，然后更加的直观查看下 es 的索引情况（或者直接敲 es 的命令）</p><p>我们用 kibana 查看存入 es 的索引如下：</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fyrgqg03bxj31en0u07op.jpg" alt=""></p><p>程序执行了一会，存入 es 的数据量就很大了。</p><h3 id="扩展配置"><a href="#扩展配置" class="headerlink" title="扩展配置"></a>扩展配置</h3><p>上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这里 flink 自带的 es sink 就支持了，常用的失败重试配置有:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1、bulk.flush.backoff.enable 用来表示是否开启重试机制</span><br><span class="line"></span><br><span class="line">2、bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）</span><br><span class="line"></span><br><span class="line">3、bulk.flush.backoff.delay 进行重试的时间间隔</span><br><span class="line"></span><br><span class="line">4、bulk.flush.backoff.retries 失败重试的次数</span><br><span class="line"></span><br><span class="line">5、bulk.flush.max.actions: 批量写入时的最大写入条数</span><br><span class="line"></span><br><span class="line">6、bulk.flush.max.size.mb: 批量写入时的最大数据量</span><br><span class="line"></span><br><span class="line">7、bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置</span><br></pre></td></tr></table></figure><p>看下啦，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充了。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fyrh2pg6v5j31c00sqjz4.jpg" alt=""></p><h3 id="FailureHandler-失败处理器"><a href="#FailureHandler-失败处理器" class="headerlink" title="FailureHandler 失败处理器"></a>FailureHandler 失败处理器</h3><p>写入 ES 的时候会有这些情况会导致写入 ES 失败：</p><p>1、ES 集群队列满了，报如下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]]</span><br></pre></td></tr></table></figure><p>是这样的，我电脑安装的 es 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 es 队列满了而写入失败。</p><p>当然这里你也可以通过调大点 es 的队列。参考：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html</a></p><p>2、ES 集群某个节点挂了</p><p>这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fys7xhkfepj30v40u0wma.jpg" alt=""></p><p>3、ES 集群某个节点的磁盘满了</p><p>这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 es 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fyrz92tqqtj31yh0k67wh.jpg" alt=""></p><p>这里我看了下源码如下图：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fyrzf1o29uj316k0u04ez.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fyrzjvr1m4j31eg0ayq5o.jpg" alt=""></p><p>如果你想继续让 es 写入的话就需要去重新配一下 es 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.addSink(<span class="keyword">new</span> ElasticsearchSink&lt;&gt;(</span><br><span class="line">    config, transportAddresses,</span><br><span class="line">    <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;,</span><br><span class="line">    <span class="keyword">new</span> ActionRequestFailureHandler() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(ActionRequest action,</span></span></span><br><span class="line"><span class="function"><span class="params">                Throwable failure,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> restStatusCode,</span></span></span><br><span class="line"><span class="function"><span class="params">                RequestIndexer indexer)</span> throw Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123;</span><br><span class="line">                <span class="comment">// full queue; re-add document for indexing</span></span><br><span class="line">                indexer.add(action);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123;</span><br><span class="line">                <span class="comment">// malformed document; simply drop request without failing sink</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// for all other failures, fail the sink</span></span><br><span class="line">                <span class="comment">// here the failure is simply rethrown, but users can also choose to throw custom exceptions</span></span><br><span class="line">                <span class="keyword">throw</span> failure;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器(failure handler)，那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文写了 Flink connector es，将 Kafka 中的数据读取并存储到 ElasticSearch 中，文中讲了如何封装自带的 sink，然后一些扩展配置以及 FailureHandler 情况下要怎么处理。（这个问题可是线上很容易遇到的）</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fy7f02y3nwj30zk0npn22.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="ElasticSearch" scheme="http://www.54tianzhisheng.cn/tags/ElasticSearch/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 中几种 Time 详解</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/11/Flink-time/</id>
    <published>2018-12-10T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy7ezqf9swj30zk0ntwiw.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 在流程序中支持不同的 <strong>Time</strong> 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。</p><p>下面我们一起来看看这几个 Time：</p><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p>Processing Time 是指事件被处理时机器的系统时间。</p><p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p><p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time  水印，这是表示 Event Time  进度的机制。 </p><p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p><p>假设所有数据都已到达， Event Time  操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 </p><p>请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。</p><h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。</p><p>在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><p>说了这么多概念比较干涩，下面直接看图：</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy8etvwp2fj31kt0u04nq.jpg" alt=""></p><h3 id="设定时间特性"><a href="#设定时间特性" class="headerlink" title="设定时间特性"></a>设定时间特性</h3><p>Flink DataStream 程序的第一部分通常是设置基本时间特性。 该设置定义了数据流源的行为方式（例如：它们是否将分配时间戳），以及像 <code>KeyedStream.timeWindow(Time.seconds(30))</code> 这样的窗口操作应该使用上面哪种时间概念。</p><p>以下示例显示了一个 Flink 程序，该程序在每小时时间窗口中聚合事件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">    .keyBy( (event) -&gt; event.getUser() )</span><br><span class="line">    .timeWindow(Time.hours(<span class="number">1</span>))</span><br><span class="line">    .reduce( (a, b) -&gt; a.add(b) )</span><br><span class="line">    .addSink(...);</span><br></pre></td></tr></table></figure><h3 id="Event-Time-和-Watermarks"><a href="#Event-Time-和-Watermarks" class="headerlink" title="Event Time 和 Watermarks"></a>Event Time 和 Watermarks</h3><p>注意：Flink 实现了数据流模型中的许多技术。有关 Event Time 和 Watermarks 的详细介绍，请查看以下文章：</p><ul><li><a href="">https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101</a></li><li><a href="">https://research.google.com/pubs/archive/43864.pdf</a></li></ul><p>支持 Event Time 的流处理器需要一种方法来衡量 Event Time 的进度。 例如，当 Event Time 超过一小时结束时，需要通知构建每小时窗口的窗口操作符，以便操作员可以关闭正在进行的窗口。</p><p>Event Time 可以独立于 Processing Time 进行。 例如，在一个程序中，操作员的当前 Event Time 可能略微落后于 Processing Time （考虑到接收事件的延迟），而两者都以相同的速度进行。另一方面，另一个流程序可能只需要几秒钟的时间就可以处理完 Kafka Topic 中数周的 Event Time 数据。</p><p>Flink 中用于衡量 Event Time 进度的机制是 Watermarks。 Watermarks 作为数据流的一部分流动并带有时间戳 t。 Watermark（t）声明 Event Time 已到达该流中的时间 t，这意味着流中不应再有具有时间戳 t’&lt;= t 的元素（即时间戳大于或等于水印的事件）</p><p>下图显示了带有(逻辑)时间戳和内联水印的事件流。在本例中，事件是按顺序排列的(相对于它们的时间戳)，这意味着水印只是流中的周期性标记。</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy8wf47i78j31h00i27dz.jpg" alt=""></p><p>Watermark 对于无序流是至关重要的，如下所示，其中事件不按时间戳排序。通常，Watermark 是一种声明，通过流中的该点，到达某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部事件时间提前到水印的值。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fy8wsjnz4fj31m60ikwp9.jpg" alt=""></p><h3 id="平行流中的水印"><a href="#平行流中的水印" class="headerlink" title="平行流中的水印"></a>平行流中的水印</h3><p>水印是在源函数处生成的，或直接在源函数之后生成的。源函数的每个并行子任务通常独立生成其水印。这些水印定义了特定并行源处的事件时间。</p><p>当水印通过流程序时，它们会提前到达操作人员处的事件时间。当一个操作符提前它的事件时间时，它为它的后续操作符在下游生成一个新的水印。</p><p>一些操作员消耗多个输入流; 例如，一个 union，或者跟随 keyBy（…）或 partition（…）函数的运算符。 这样的操作员当前事件时间是其输入流的事件时间的最小值。 由于其输入流更新其事件时间，因此操作员也是如此。</p><p>下图显示了流经并行流的事件和水印的示例，以及跟踪事件时间的运算符。</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fy8xyb8xazj31a10u07kv.jpg" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md">https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">http://www.54tianzhisheng.cn/2018/12/11/Flink-time/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fy7ezqf9swj30zk0ntwiw.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 介绍Flink中的Stream Windows</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/</id>
    <published>2018-12-07T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy7eyu23u0j30zk0k0n2r.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语（例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” ）。</p><p>对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。</p><p>Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><p>在这篇文章中，我们将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。</p><h3 id="什么是-Windows？"><a href="#什么是-Windows？" class="headerlink" title="什么是 Windows？"></a>什么是 Windows？</h3><p>下面我们结合一个现实的例子来说明。</p><p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？ </p><p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy22ikpc75j312m02twer.jpg" alt=""></p><p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy22nm8j7pj30h3035dg1.jpg" alt=""></p><p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。</p><p>因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？<br>这个问题，就相当于一个定义了一个 Window（窗口），window 的界限是1分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： </p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fy22ocg7dij30gr03yaaf.jpg" alt=""></p><p>第一分钟的数量为8，第二分钟是22，第三分钟是27。。。这样，1个小时内会有60个window。</p><p>再考虑一种情况，每30秒统计一次过去1分钟的汽车数量之和： </p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy22pd4djhj30gq04y74w.jpg" alt=""></p><p>此时，window 出现了重合。这样，1个小时内会有120个 window。</p><p>扩展一下，我们可以在某个地区，收集每一个红绿灯处汽车经过的数量，然后每个红绿灯处都做一次基于1分钟的window统计，即并行处理： </p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy22pz8te9j30h405s3z8.jpg" alt=""></p><h3 id="它有什么作用？"><a href="#它有什么作用？" class="headerlink" title="它有什么作用？"></a>它有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。window 又可以分为基于时间（Time-based）的 window 以及基于数量（Count-based）的 window。</p><h3 id="Flink-自带的-window"><a href="#Flink-自带的-window" class="headerlink" title="Flink 自带的 window"></a>Flink 自带的 window</h3><p>Flink DataStream API 提供了 Time 和 Count 的 window，同时增加了基于 Session 的 window。同时，由于某些特殊的需要，DataStream API 也提供了定制化的 window 操作，供用户自定义 window。</p><p>下面，主要介绍 Time-Based window 以及 Count-Based window，以及自定义的 window 操作，Session-Based Window 操作将会在后续的文章中讲到。</p><h4 id="Time-Windows"><a href="#Time-Windows" class="headerlink" title="Time Windows"></a>Time Windows</h4><p>正如命名那样，Time Windows 根据时间来聚合流数据。例如：一分钟的 tumbling time window 收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于一个函数。</p><p>在 Flink 中定义 tumbling time windows(翻滚时间窗口) 和 sliding time windows(滑动时间窗口) 非常简单：</p><p><strong>tumbling time windows(翻滚时间窗口)</strong></p><p>输入一个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//tumbling time window 每分钟统计一次数量和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>sliding time windows(滑动时间窗口)</strong></p><p>输入两个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>有一点我们还没有讨论，即“收集一分钟的元素”的确切含义，它可以归结为一个问题，“流处理器如何解释时间?”</p><p>Apache Flink 具有三个不同的时间概念，即 processing time, event time 和 ingestion time。</p><p>这里可以参考我下一篇文章：</p><p><a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Event Time、Processing Time和Ingestion Time</a></p><h4 id="Count-Windows"><a href="#Count-Windows" class="headerlink" title="Count Windows"></a>Count Windows</h4><p>Apache Flink 还提供计数窗口功能。如果计数窗口设置的为 100 ，那么将会在窗口中收集 100 个事件，并在添加第 100 个元素时计算窗口的值。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fy36yi14irj317w0o678p.jpg" alt=""></p><p>在 Flink 的 DataStream API 中，tumbling count window 和 sliding count window 的定义如下:</p><p><strong>tumbling count window</strong></p><p>输入一个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.countWindow(<span class="number">100</span>) <span class="comment">//统计每 100 个元素的数量之和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>sliding count window</strong></p><p>输入两个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>) </span><br><span class="line">.countWindow(<span class="number">100</span>, <span class="number">10</span>) <span class="comment">//每 10 个元素统计过去 100 个元素的数量之和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="解剖-Flink-的窗口机制"><a href="#解剖-Flink-的窗口机制" class="headerlink" title="解剖 Flink 的窗口机制"></a>解剖 Flink 的窗口机制</h4><p>Flink 的内置 time window 和 count window 已经覆盖了大多数应用场景，但是有时候也需要定制窗口逻辑，此时 Flink 的内置的 window 无法解决这些问题。为了还支持自定义 window 实现不同的逻辑，DataStream API 为其窗口机制提供了接口。</p><p>下图描述了 Flink 的窗口机制，并介绍了所涉及的组件：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fy320tli71j312t0nfgpt.jpg" alt=""></p><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。<br>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。</p><p>每个窗口都拥有一个 Trigger(触发器)，该 Trigger(触发器) 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发(即、清除(删除窗口并丢弃其内容)，或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p><p>当 Trigger(触发器) 触发时，可以将窗口元素列表提供给可选的 Evictor，Evictor 可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素。然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p><p>计算函数接收 Evictor 过滤后的窗口元素，并计算窗口的一个或多个元素的结果。 DataStream API 接受不同类型的计算函数，包括预定义的聚合函数，如 sum（），min（），max（），以及 ReduceFunction，FoldFunction 或 WindowFunction。</p><p>这些是构成 Flink 窗口机制的组件。 接下来我们逐步演示如何使用 DataStream API 实现自定义窗口逻辑。 我们从 DataStream [IN] 类型的流开始，并使用 key 选择器函数对其分组，该函数将 key 相同类型的数据分组在一块。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;xxx&gt; data = env.addSource(...);</span><br><span class="line">data.keyBy()</span><br></pre></td></tr></table></figure><h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>1、Window Assigner</p><p>负责将元素分配到不同的 window。</p><p>Window API 提供了自定义的 WindowAssigner 接口，我们可以实现 WindowAssigner 的 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp)</span></span></span><br></pre></td></tr></table></figure><p>方法。同时，对于基于 Count 的 window 而言，默认采用了 GlobalWindow 的 window assigner，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyBy.window(GlobalWindows.create())</span><br></pre></td></tr></table></figure><p>2、Trigger </p><p>Trigger 即触发器，定义何时或什么情况下移除 window</p><p>我们可以指定触发器来覆盖 WindowAssigner 提供的默认触发器。 请注意，指定的触发器不会添加其他触发条件，但会替换当前触发器。</p><p>3、Evictor（可选）</p><p>驱逐者，即保留上一 window 留下的某些元素</p><p>4、通过 apply WindowFunction 来返回 DataStream 类型数据。</p><p>利用 Flink 的内部窗口机制和 DataStream API 可以实现自定义的窗口逻辑，例如 session window。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>对于现代流处理器来说，支持连续数据流上的各种类型的窗口是必不可少的。 Apache Flink 是一个具有强大功能集的流处理器，包括一个非常灵活的机制，可以在连续数据流上构建窗口。 Flink 为常见场景提供内置的窗口运算符，以及允许用户自定义窗口逻辑。 </p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1、<a href="">https://flink.apache.org/news/2015/12/04/Introducing-windows.html</a></p><p>2、<a href="">https://blog.csdn.net/lmalds/article/details/51604501</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNbRwly1fy7eyu23u0j30zk0k0n2r.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink Data transformation(转换)</title>
    <link href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/"/>
    <id>http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/</id>
    <published>2018-11-03T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fx6uzoige7j31hc0zbtjz.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在第一篇介绍 Flink 的文章 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/#Flink-%E7%A8%8B%E5%BA%8F%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81%E7%BB%93%E6%9E%84">《《从0到1学习Flink》—— Apache Flink 介绍》</a> 中就说过 Flink 程序的结构</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw7ij4194jj31a00usq7o.jpg" alt=""></p><p>Flink 应用程序结构就是如上图所示：</p><p>1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p><p>2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p><p>3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。</p><p>在上四篇文章介绍了 Source 和 Sink：</p><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>那么这篇文章我们就来看下 Flink Data Transformation 吧，数据转换操作还是蛮多的，需要好好讲讲！</p><h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><p>这是最简单的转换之一，其中输入是一个数据流，输出的也是一个数据流：</p><p>还是拿上一篇文章的案例来将数据进行 map 转换操作：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Student&gt; map = student.map(<span class="keyword">new</span> MapFunction&lt;Student, Student&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Student <span class="title">map</span><span class="params">(Student value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Student s1 = <span class="keyword">new</span> Student();</span><br><span class="line">        s1.id = value.id;</span><br><span class="line">        s1.name = value.name;</span><br><span class="line">        s1.password = value.password;</span><br><span class="line">        s1.age = value.age + <span class="number">5</span>;</span><br><span class="line">        <span class="keyword">return</span> s1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">map.print();</span><br></pre></td></tr></table></figure><p>将每个人的年龄都增加 5 岁，其他不变。</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwwfyjd5x0j31kw0zkqk9.jpg" alt=""></p><h4 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h4><p>FlatMap 采用一条记录并输出零个，一个或多个记录。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Student&gt; flatMap = student.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Student, Student&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(Student value, Collector&lt;Student&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.id % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            out.collect(value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">flatMap.print();</span><br></pre></td></tr></table></figure><p>这里将 id 为偶数的聚集出来。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwwgjsk84vj31kw0zkh43.jpg" alt=""></p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><p>Filter 函数根据条件判断出结果。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Student&gt; filter = student.filter(<span class="keyword">new</span> FilterFunction&lt;Student&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Student value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (value.id &gt; <span class="number">95</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">filter.print();</span><br></pre></td></tr></table></figure><p>这里将 id 大于 95 的过滤出来，然后打印出来。</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwwgt9xud1j31kw0zktnh.jpg" alt=""></p><h4 id="KeyBy"><a href="#KeyBy" class="headerlink" title="KeyBy"></a>KeyBy</h4><p>KeyBy 在逻辑上是基于 key 对流进行分区。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream&lt;Student, Integer&gt; keyBy = student.keyBy(<span class="keyword">new</span> KeySelector&lt;Student, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Student value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">keyBy.print();</span><br></pre></td></tr></table></figure><p>上面对 student 的 age 做 KeyBy 操作分区</p><h4 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h4><p>Reduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average, sum, min, max, count，使用 reduce 方法都可实现。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Student&gt; reduce = student.keyBy(<span class="keyword">new</span> KeySelector&lt;Student, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getKey</span><span class="params">(Student value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).reduce(<span class="keyword">new</span> ReduceFunction&lt;Student&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Student <span class="title">reduce</span><span class="params">(Student value1, Student value2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Student student1 = <span class="keyword">new</span> Student();</span><br><span class="line">        student1.name = value1.name + value2.name;</span><br><span class="line">        student1.id = (value1.id + value2.id) / <span class="number">2</span>;</span><br><span class="line">        student1.password = value1.password + value2.password;</span><br><span class="line">        student1.age = (value1.age + value2.age) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">return</span> student1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">reduce.print();</span><br></pre></td></tr></table></figure><p>上面先将数据流进行 keyby 操作，因为执行 reduce 操作只能是 KeyedStream，然后将 student 对象的 age 做了一个求平均值的操作。</p><h4 id="Fold"><a href="#Fold" class="headerlink" title="Fold"></a>Fold</h4><p>Fold 通过将最后一个文件夹流与当前记录组合来推出 KeyedStream。 它会发回数据流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream.fold(<span class="string">"1"</span>, <span class="keyword">new</span> FoldFunction&lt;Integer, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">fold</span><span class="params">(String accumulator, Integer value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator + <span class="string">"="</span> + value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h4><p>DataStream API 支持各种聚合，例如 min，max，sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KeyedStream.sum(<span class="number">0</span>) </span><br><span class="line">KeyedStream.sum(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.min(<span class="number">0</span>) </span><br><span class="line">KeyedStream.min(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.max(<span class="number">0</span>) </span><br><span class="line">KeyedStream.max(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.minBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.minBy(<span class="string">"key"</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="number">0</span>) </span><br><span class="line">KeyedStream.maxBy(<span class="string">"key"</span>)</span><br></pre></td></tr></table></figure><p>max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理。</p><h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><p>Window 函数允许按时间或其他条件对现有 KeyedStream 进行分组。 以下是以 10 秒的时间窗口聚合：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).window(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><p>Flink 定义数据片段以便（可能）处理无限数据流。 这些切片称为窗口。 此切片有助于通过应用转换处理数据块。 要对流进行窗口化，我们需要分配一个可以进行分发的键和一个描述要对窗口化流执行哪些转换的函数</p><p>要将流切片到窗口，我们可以使用 Flink 自带的窗口分配器。 我们有选项，如 tumbling windows, sliding windows, global 和 session windows。 Flink 还允许您通过扩展 WindowAssginer 类来编写自定义窗口分配器。 这里先预留下篇文章来讲解这些不同的 windows 是如何工作的。</p><h4 id="WindowAll"><a href="#WindowAll" class="headerlink" title="WindowAll"></a>WindowAll</h4><p>windowAll 函数允许对常规数据流进行分组。 通常，这是非并行数据转换，因为它在非分区数据流上运行。</p><p>与常规数据流功能类似，我们也有窗口数据流功能。 唯一的区别是它们处理窗口数据流。 所以窗口缩小就像 Reduce 函数一样，Window fold 就像 Fold 函数一样，并且还有聚合。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.keyBy(<span class="number">0</span>).windowAll(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h4 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h4><p>Union 函数将两个或多个数据流结合在一起。 这样就可以并行地组合数据流。 如果我们将一个流与自身组合，那么它会输出每个记录两次。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputStream.union(inputStream1, inputStream2, ...);</span><br></pre></td></tr></table></figure><h4 id="Window-join"><a href="#Window-join" class="headerlink" title="Window join"></a>Window join</h4><p>我们可以通过一些 key 将同一个 window 的两个数据流 join 起来。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputStream.join(inputStream1)</span><br><span class="line">           .where(<span class="number">0</span>).equalTo(<span class="number">1</span>)</span><br><span class="line">           .window(Time.seconds(<span class="number">5</span>))     </span><br><span class="line">           .apply (<span class="keyword">new</span> JoinFunction () &#123;...&#125;);</span><br></pre></td></tr></table></figure><p>以上示例是在 5 秒的窗口中连接两个流，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。</p><h4 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h4><p>此功能根据条件将流拆分为两个或多个流。 当您获得混合流并且您可能希望单独处理每个数据流时，可以使用此方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split = inputStream.split(<span class="keyword">new</span> OutputSelector&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(Integer value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; output = <span class="keyword">new</span> ArrayList&lt;String&gt;(); </span><br><span class="line">        <span class="keyword">if</span> (value % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">            output.add(<span class="string">"even"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            output.add(<span class="string">"odd"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h4 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h4><p>此功能允许您从拆分流中选择特定流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SplitStream&lt;Integer&gt; split;</span><br><span class="line">DataStream&lt;Integer&gt; even = split.select(<span class="string">"even"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; odd = split.select(<span class="string">"odd"</span>); </span><br><span class="line">DataStream&lt;Integer&gt; all = split.select(<span class="string">"even"</span>,<span class="string">"odd"</span>);</span><br></pre></td></tr></table></figure><h4 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h4><p>Project 函数允许您从事件流中选择属性子集，并仅将所选元素发送到下一个处理流。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;Tuple4&lt;Integer, Double, String, String&gt;&gt; in = <span class="comment">// [...] </span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, String&gt;&gt; out = in.project(<span class="number">3</span>,<span class="number">2</span>);</span><br></pre></td></tr></table></figure><p>上述函数从给定记录中选择属性号 2 和 3。 以下是示例输入和输出记录：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>,<span class="number">10.0</span>,A,B)=&gt; (B,A)</span><br><span class="line">(<span class="number">2</span>,<span class="number">20.0</span>,C,D)=&gt; (D,C)</span><br></pre></td></tr></table></figure><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要介绍了 Flink Data 的常用转换方式：Map、FlatMap、Filter、KeyBy、Reduce、Fold、Aggregations、Window、WindowAll、Union、Window Join、Split、Select、Project 等。并用了点简单的 demo 介绍了如何使用，具体在项目中该如何将数据流转换成我们想要的格式，还需要根据实际情况对待。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fx6uzoige7j31hc0zbtjz.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 如何自定义 Data Sink ？</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/</id>
    <published>2018-10-30T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwuyra5n0oj31hc0zl44z.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前篇文章 <a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a> 介绍了 Flink Data Sink，也介绍了 Flink 自带的 Sink，那么如何自定义自己的 Sink 呢？这篇文章将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去。</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。</p><p>运行启动 Flink、Zookepeer、Kafka，</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwuuz88g2oj31kw0d5gt5.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwuv1xdaq5j31kw08zq7u.jpg" alt=""></p><p>好了，都启动了！</p><h3 id="数据库建表"><a href="#数据库建表" class="headerlink" title="数据库建表"></a>数据库建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure><h3 id="实体类"><a href="#实体类" class="headerlink" title="实体类"></a>实体类</h3><p>Student.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink.model;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixin: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> String password;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(<span class="keyword">int</span> id, String name, String password, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span> +</span><br><span class="line">                <span class="string">"id="</span> + id +</span><br><span class="line">                <span class="string">", name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", password='"</span> + password + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", age="</span> + age +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPassword</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h3><p>工具类往 kafka topic student 发送数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Metric;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据</span></span><br><span class="line"><span class="comment"> * 可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> * weixin: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils2</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(student));</span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SinkToMySQL"><a href="#SinkToMySQL" class="headerlink" title="SinkToMySQL"></a>SinkToMySQL</h3><p>该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法。在 invoke 方法中将数据插入到 MySQL 中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixin: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(Student value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//组装数据，执行插入操作</span></span><br><span class="line">        ps.setInt(<span class="number">1</span>, value.getId());</span><br><span class="line">        ps.setString(<span class="number">2</span>, value.getName());</span><br><span class="line">        ps.setString(<span class="number">3</span>, value.getPassword());</span><br><span class="line">        ps.setInt(<span class="number">4</span>, value.getAge());</span><br><span class="line">        ps.executeUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">            con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h3><p>这里的 source 是从 kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Student;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.sink.SinkToMySQL;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.PrintSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixin: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main3</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; JSON.parseObject(string, Student.class)); <span class="comment">//Fastjson 解析字符串成 student 对象</span></span><br><span class="line"></span><br><span class="line">        student.addSink(<span class="keyword">new</span> SinkToMySQL()); <span class="comment">//数据 sink 到 mysql</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add sink"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>运行 Flink 程序，然后再运行 KafkaUtils2.java 工具类，这样就可以了。</p><p>如果数据插入成功了，那么我们查看下我们的数据库：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwvy09fk5rj31kw0zktpo.jpg" alt=""></p><p>数据库中已经插入了 100 条我们从 Kafka 发送的数据了。证明我们的 SinkToMySQL 起作用了。是不是很简单？</p><h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h3><p>怕大家不知道我的项目结构，这里发个截图看下：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwvy1f10vqj31kw0zkh2m.jpg" alt=""></p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要利用一个 demo，告诉大家如何自定义 Sink Function，将从 Kafka 的数据 Sink 到 MySQL 中，如果你项目中有其他的数据来源，你也可以换成对应的 Source，也有可能你的 Sink 是到其他的地方或者其他不同的方式，那么依旧是这个套路：继承 RichSinkFunction 抽象类，重写 invoke 方法。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fwuyra5n0oj31hc0zl44z.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 如何自定义 Data Source ？</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/</id>
    <published>2018-10-29T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwuyqgvyqnj31hc19lk79.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在 <a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a> 文章中，我给大家介绍了 Flink Data Source 以及简短的介绍了一下自定义 Data Source，这篇文章更详细的介绍下，并写一个 demo 出来让大家理解。</p><h3 id="Flink-Kafka-source"><a href="#Flink-Kafka-source" class="headerlink" title="Flink Kafka source"></a>Flink Kafka source</h3><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。</p><p>运行启动 Flink、Zookepeer、Kafka，</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwuuz88g2oj31kw0d5gt5.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwuv1xdaq5j31kw08zq7u.jpg" alt=""></p><p>好了，都启动了！</p><h4 id="maven-依赖"><a href="#maven-依赖" class="headerlink" title="maven 依赖"></a>maven 依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--flink java--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--日志--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--flink kafka connector--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--alibaba fastjson--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.51<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="测试发送数据到-kafka-topic"><a href="#测试发送数据到-kafka-topic" class="headerlink" title="测试发送数据到 kafka topic"></a>测试发送数据到 kafka topic</h4><p>实体类，Metric.java </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink.model;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixi: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Metric</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, Object&gt; fields;</span><br><span class="line">    <span class="keyword">public</span> Map&lt;String, String&gt; tags;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Metric</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Metric</span><span class="params">(String name, <span class="keyword">long</span> timestamp, Map&lt;String, Object&gt; fields, Map&lt;String, String&gt; tags)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">        <span class="keyword">this</span>.fields = fields;</span><br><span class="line">        <span class="keyword">this</span>.tags = tags;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Metric&#123;"</span> +</span><br><span class="line">                <span class="string">"name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", timestamp='"</span> + timestamp + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", fields="</span> + fields +</span><br><span class="line">                <span class="string">", tags="</span> + tags +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getTimestamp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTimestamp</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.timestamp = timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, Object&gt; <span class="title">getFields</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> fields;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFields</span><span class="params">(Map&lt;String, Object&gt; fields)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.fields = fields;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Map&lt;String, String&gt; <span class="title">getTags</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTags</span><span class="params">(Map&lt;String, String&gt; tags)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tags = tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>往 kafka 中写数据工具类：KafkaUtils.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Metric;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 往kafka中写数据</span></span><br><span class="line"><span class="comment"> * 可以使用这个main函数进行测试一下</span></span><br><span class="line"><span class="comment"> * weixin: zhisheng_tian </span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtils</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"metric"</span>;  <span class="comment">// kafka topic，Flink 程序中需要和这个统一 </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//key 序列化</span></span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>); <span class="comment">//value 序列化</span></span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        Metric metric = <span class="keyword">new</span> Metric();</span><br><span class="line">        metric.setTimestamp(System.currentTimeMillis());</span><br><span class="line">        metric.setName(<span class="string">"mem"</span>);</span><br><span class="line">        Map&lt;String, String&gt; tags = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Map&lt;String, Object&gt; fields = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        tags.put(<span class="string">"cluster"</span>, <span class="string">"zhisheng"</span>);</span><br><span class="line">        tags.put(<span class="string">"host_ip"</span>, <span class="string">"101.147.022.106"</span>);</span><br><span class="line"></span><br><span class="line">        fields.put(<span class="string">"used_percent"</span>, <span class="number">90</span>d);</span><br><span class="line">        fields.put(<span class="string">"max"</span>, <span class="number">27244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"used"</span>, <span class="number">17244873</span>d);</span><br><span class="line">        fields.put(<span class="string">"init"</span>, <span class="number">27244873</span>d);</span><br><span class="line"></span><br><span class="line">        metric.setTags(tags);</span><br><span class="line">        metric.setFields(fields);</span><br><span class="line"></span><br><span class="line">        ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, JSON.toJSONString(metric));</span><br><span class="line">        producer.send(record);</span><br><span class="line">        System.out.println(<span class="string">"发送数据: "</span> + JSON.toJSONString(metric));</span><br><span class="line"></span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">300</span>);</span><br><span class="line">            writeToKafka();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行：</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwuv8pz2dmj31kw0zk7lu.jpg" alt=""></p><p>如果出现如上图标记的，即代表能够不断的往 kafka 发送数据的。</p><h4 id="Flink-程序"><a href="#Flink-程序" class="headerlink" title="Flink 程序"></a>Flink 程序</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixi: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);  <span class="comment">//key 反序列化</span></span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>); <span class="comment">//value 反序列化</span></span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"metric"</span>,  <span class="comment">//kafka topic</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),  <span class="comment">// String 序列化</span></span><br><span class="line">                props)).setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        dataStreamSource.print(); <span class="comment">//把从 kafka 读取到的数据打印在控制台</span></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data source"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行起来：</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwuvbauvvgj31kw0zkqso.jpg" alt=""></p><p>看到没程序，Flink 程序控制台能够源源不断的打印数据呢。</p><h3 id="自定义-Source"><a href="#自定义-Source" class="headerlink" title="自定义 Source"></a>自定义 Source</h3><p>上面就是 Flink 自带的 Kafka source，那么接下来就模仿着写一个从 MySQL 中读取数据的 Source。</p><p>首先 pom.xml 中<strong>添加 MySQL 依赖</strong>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>数据库建表</strong>如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> <span class="string">`student`</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`student`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`password`</span> <span class="built_in">varchar</span>(<span class="number">25</span>) <span class="keyword">COLLATE</span> utf8_bin <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`age`</span> <span class="built_in">int</span>(<span class="number">10</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">5</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COLLATE</span>=utf8_bin;</span><br></pre></td></tr></table></figure><p><strong>插入数据</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`student`</span> <span class="keyword">VALUES</span> (<span class="string">'1'</span>, <span class="string">'zhisheng01'</span>, <span class="string">'123456'</span>, <span class="string">'18'</span>), (<span class="string">'2'</span>, <span class="string">'zhisheng02'</span>, <span class="string">'123'</span>, <span class="string">'17'</span>), (<span class="string">'3'</span>, <span class="string">'zhisheng03'</span>, <span class="string">'1234'</span>, <span class="string">'18'</span>), (<span class="string">'4'</span>, <span class="string">'zhisheng04'</span>, <span class="string">'12345'</span>, <span class="string">'16'</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure><p><strong>新建实体类</strong>：Student.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink.model;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixi: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">    <span class="keyword">public</span> String password;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(<span class="keyword">int</span> id, String name, String password, <span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span> +</span><br><span class="line">                <span class="string">"id="</span> + id +</span><br><span class="line">                <span class="string">", name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", password='"</span> + password + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", age="</span> + age +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPassword</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPassword</span><span class="params">(String password)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.password = password;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>新建 Source 类</strong> SourceFromMySQL.java，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法：</p> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">package</span> com.zhisheng.flink.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.RichSourceFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixi: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceFromMySQL</span> <span class="keyword">extends</span> <span class="title">RichSourceFunction</span>&lt;<span class="title">Student</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        connection = getConnection();</span><br><span class="line">        String sql = <span class="string">"select * from Student;"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 程序执行完毕就可以进行，关闭连接和释放资源的动作了</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123; <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * DataStream 调用一次 run() 方法用来获取数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> ctx</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;Student&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ResultSet resultSet = ps.executeQuery();</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(</span><br><span class="line">                    resultSet.getInt(<span class="string">"id"</span>),</span><br><span class="line">                    resultSet.getString(<span class="string">"name"</span>).trim(),</span><br><span class="line">                    resultSet.getString(<span class="string">"password"</span>).trim(),</span><br><span class="line">                    resultSet.getInt(<span class="string">"age"</span>));</span><br><span class="line">            ctx.collect(student);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">                con = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8"</span>, <span class="string">"root"</span>, <span class="string">"root123456"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span>+ e.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Flink 程序</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.flink.source.SourceFromMySQL;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc:</span></span><br><span class="line"><span class="comment"> * weixi: zhisheng_tian</span></span><br><span class="line"><span class="comment"> * blog: http://www.54tianzhisheng.cn/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        env.addSource(<span class="keyword">new</span> SourceFromMySQL()).print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Flink add data sourc"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 Flink 程序，控制台日志中可以看见打印的 student 信息。</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwuxfzfmvyj31kw0zknll.jpg" alt=""></p><h3 id="RichSourceFunction"><a href="#RichSourceFunction" class="headerlink" title="RichSourceFunction"></a>RichSourceFunction</h3><p>从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，那么来了解一下：</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwuxklp906j31ki122n3n.jpg" alt=""></p><p>一个抽象类，继承自 AbstractRichFunction。为实现一个 Rich SourceFunction 提供基础能力。该类的子类有三个，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwuy2kb4w1j31kw0tcdpj.jpg" alt=""></p><ul><li>MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。</li><li>MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。</li><li>ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。</li></ul><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要讲了下 Flink 使用 Kafka Source 的使用，并提供了一个 demo 教大家如何自定义 Source，从 MySQL 中读取数据，当然你也可以从其他地方读取，实现自己的数据源 source。可能平时工作会比这个更复杂，需要大家灵活应对！</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/</a><br>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fwuyqgvyqnj31hc19lk79.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Data Sink 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/29/flink-sink/</id>
    <published>2018-10-28T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwuyp39rdsj31hc0zkdm5.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>再上一篇文章中 <a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a> 讲解了 Flink Data Source ，那么这里就来讲讲 Flink Data Sink 吧。</p><p>首先 Sink 的意思是：</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwup3gne27j30w20ridk6.jpg" alt=""></p><p>大概可以猜到了吧！Data sink 有点把数据存储下来（落库）的意思。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwup8qbbsxj31580fw0sy.jpg" alt=""></p><p>如上图，Source 就是数据的来源，中间的 Compute 其实就是 Flink 干的事情，可以做一系列的操作，操作完后就把计算后的数据结果 Sink 到某个地方。（可以是 MySQL、ElasticSearch、Kafka、Cassandra 等）。这里我说下自己目前做告警这块就是把 Compute 计算后的结果 Sink 直接告警出来了（发送告警消息到钉钉群、邮件、短信等），这个 sink 的意思也不一定非得说成要把数据存储到某个地方去。其实官网用的 Connector 来形容要去的地方更合适，这个 Connector 可以有 MySQL、ElasticSearch、Kafka、Cassandra RabbitMQ 等。 </p><h3 id="Flink-Data-Sink"><a href="#Flink-Data-Sink" class="headerlink" title="Flink Data Sink"></a>Flink Data Sink</h3><p>前面文章 <a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a> 介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwvrzt3qvhj31kw0xgtol.jpg" alt=""></p><p>看下源码有哪些呢？</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwvsf507rtj31kw076diy.jpg" alt=""></p><p>可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。</p><h3 id="SinkFunction"><a href="#SinkFunction" class="headerlink" title="SinkFunction"></a>SinkFunction</h3><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwvtgc8sfmj31060mqwfo.jpg" alt=""></p><p>从上图可以看到 SinkFunction 接口有 invoke 方法，它有一个 RichSinkFunction 抽象类。</p><p>上面的那些自带的 Sink 可以看到都是继承了 RichSinkFunction 抽象类，实现了其中的方法，那么我们要是自己定义自己的 Sink 的话其实也是要按照这个套路来做的。</p><p>这里就拿个较为简单的 PrintSinkFunction 源码来讲下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PrintSinkFunction</span>&lt;<span class="title">IN</span>&gt; <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">IN</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> STD_OUT = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> STD_ERR = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">boolean</span> target;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> PrintStream stream;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">transient</span> String prefix;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Instantiates a print sink function that prints to standard out.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PrintSinkFunction</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Instantiates a print sink function that prints to standard out.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> stdErr True, if the format should print to standard error instead of standard out.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PrintSinkFunction</span><span class="params">(<span class="keyword">boolean</span> stdErr)</span> </span>&#123;</span><br><span class="line">target = stdErr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTargetToStandardOut</span><span class="params">()</span> </span>&#123;</span><br><span class="line">target = STD_OUT;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setTargetToStandardErr</span><span class="params">()</span> </span>&#123;</span><br><span class="line">target = STD_ERR;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">super</span>.open(parameters);</span><br><span class="line">StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext();</span><br><span class="line"><span class="comment">// get the target stream</span></span><br><span class="line">stream = target == STD_OUT ? System.out : System.err;</span><br><span class="line"></span><br><span class="line"><span class="comment">// set the prefix if we have a &gt;1 parallelism</span></span><br><span class="line">prefix = (context.getNumberOfParallelSubtasks() &gt; <span class="number">1</span>) ?</span><br><span class="line">((context.getIndexOfThisSubtask() + <span class="number">1</span>) + <span class="string">"&gt; "</span>) : <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN record)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (prefix != <span class="keyword">null</span>) &#123;</span><br><span class="line">stream.println(prefix + record.toString());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">stream.println(record.toString());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.stream = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">this</span>.prefix = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"Print to "</span> + (target == STD_OUT ? <span class="string">"System.out"</span> : <span class="string">"System.err"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到它就是实现了 RichSinkFunction 抽象类，然后实现了 invoke 方法，这里 invoke 方法就是把记录打印出来了就是，没做其他的额外操作。</p><h3 id="如何使用？"><a href="#如何使用？" class="headerlink" title="如何使用？"></a>如何使用？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator.addSink(<span class="keyword">new</span> PrintSinkFunction&lt;&gt;();</span><br></pre></td></tr></table></figure><p>这样就可以了，如果是其他的 Sink Function 的话需要换成对应的。</p><p>使用这个 Function 其效果就是打印从 Source 过来的数据，和直接 Source.print() 效果一样。</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fwvu5uahrwj31kw0xdaph.jpg" alt=""></p><p>下篇文章我们将讲解下如何自定义自己的 Sink Function，并使用一个 demo 来教大家，让大家知道这个套路，且能够在自己工作中自定义自己需要的 Sink Function，来完成自己的工作需求。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要讲了下 Flink 的 Data Sink，并介绍了常见的 Data Sink，也看了下源码的 SinkFunction，介绍了一个简单的 Function 使用, 告诉了大家自定义 Sink Function 的套路，下篇文章带大家写个。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">http://www.54tianzhisheng.cn/2018/10/29/flink-sink/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNbRwly1fwuyp39rdsj31hc0zkdm5.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Data Source 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/28/flink-sources/</id>
    <published>2018-10-27T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwo9x3xvavj31hc0zkalc.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Data Sources 是什么呢？就字面意思其实就可以知道：数据来源。</p><p>Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即实时的处理些实时数据流，实时的产生数据流结果，只要数据源源不断的过来，Flink 就能够一直计算下去，这个 Data Sources 就是数据的来源地。</p><p>Flink 中你可以使用 <code>StreamExecutionEnvironment.addSource(sourceFunction)</code> 来为你的程序添加数据来源。</p><p>Flink 已经提供了若干实现好了的 source functions，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source，</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>StreamExecutionEnvironment 中可以使用以下几个已实现的 stream sources，</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwunb07b35j31kw16px6q.jpg" alt=""></p><p>总的来说可以分为下面几大类：</p><h4 id="基于集合"><a href="#基于集合" class="headerlink" title="基于集合"></a>基于集合</h4><p>1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。</p><p>2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。</p><p>3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Event&gt; input = env.fromElements(</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">1</span>, <span class="string">"barfoo"</span>, <span class="number">1.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">2</span>, <span class="string">"start"</span>, <span class="number">2.0</span>),</span><br><span class="line"><span class="keyword">new</span> Event(<span class="number">3</span>, <span class="string">"foobar"</span>, <span class="number">3.0</span>),</span><br><span class="line">...</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。</p><p>5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。</p><h4 id="基于文件"><a href="#基于文件" class="headerlink" title="基于文件"></a>基于文件</h4><p>1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;String&gt; text = env.readTextFile(<span class="string">"file:///path/to/file"</span>);</span><br></pre></td></tr></table></figure><p>2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。</p><p>3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.readFile(</span><br><span class="line">        myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, <span class="number">100</span>,</span><br><span class="line">        FilePathFilter.createDefaultFilter(), typeInfo);</span><br></pre></td></tr></table></figure><p>实现:</p><p>在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。</p><p>重要注意：</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。</p><p>如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。</p><h4 id="基于-Socket："><a href="#基于-Socket：" class="headerlink" title="基于 Socket："></a>基于 Socket：</h4><p>socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env</span><br><span class="line">        .socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>) <span class="comment">// 监听 localhost 的 9999 端口过来的数据</span></span><br><span class="line">        .flatMap(<span class="keyword">new</span> Splitter())</span><br><span class="line">        .keyBy(<span class="number">0</span>)</span><br><span class="line">        .timeWindow(Time.seconds(<span class="number">5</span>))</span><br><span class="line">        .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>这个在 <a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a> 文章里用的就是基于 Socket 的 Word Count 程序。</p><h4 id="自定义："><a href="#自定义：" class="headerlink" title="自定义："></a>自定义：</h4><p>addSource - 添加一个新的 source function。例如，你可以 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 以从 Apache Kafka 读取数据</p><p><strong>说下上面几种的特点吧</strong>：</p><p>1、基于集合：有界数据集，更偏向于本地测试用</p><p>2、基于文件：适合监听文件修改并读取其内容</p><p>3、基于 Socket：监听主机的 host port，从 Socket 中获取数据</p><p>4、自定义 addSource：大多数的场景数据都是无界的，会源源不断的过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">DataStream&lt;KafkaEvent&gt; input = env</span><br><span class="line">.addSource(</span><br><span class="line"><span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">parameterTool.getRequired(<span class="string">"input-topic"</span>), <span class="comment">//从参数中获取传进来的 topic </span></span><br><span class="line"><span class="keyword">new</span> KafkaEventSchema(),</span><br><span class="line">parameterTool.getProperties())</span><br><span class="line">.assignTimestampsAndWatermarks(<span class="keyword">new</span> CustomWatermarkExtractor()));</span><br></pre></td></tr></table></figure><p>Flink 目前支持如下图里面常见的 Source：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fwuo5tsgi0j31kw0ye7jt.jpg" alt=""></p><p>如果你想自己自定义自己的 Source 呢？</p><p>那么你就需要去了解一下 SourceFunction 接口了，它是所有 stream source 的根接口，它继承自一个标记接口（空接口）Function。</p><p>SourceFunction 定义了两个接口方法：</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwunmyv9ewj30uy0lodh0.jpg" alt=""></p><p>1、run ： 启动一个 source，即对接一个外部数据源然后 emit 元素形成 stream（大部分情况下会通过在该方法里运行一个 while 循环的形式来产生 stream）。</p><p>2、cancel ： 取消一个 source，也即将 run 中的循环 emit 元素的行为终止。</p><p>正常情况下，一个 SourceFunction 实现这两个接口方法就可以了。其实这两个接口方法也固定了一种实现模板。</p><p>比如，实现一个 XXXSourceFunction，那么大致的模板是这样的：(直接拿 FLink 源码的实例给你看看)</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwunrguyfej31kw0yfter.jpg" alt=""></p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要讲了下 Flink 的常见 Source 有哪些并且简单的提了下如何自定义 Source。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">http://www.54tianzhisheng.cn/2018/10/28/flink-sources/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fwo9x3xvavj31hc0zkalc.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 配置文件详解</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/27/flink-config/</id>
    <published>2018-10-26T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwo0tys9m5j31hc0zfncl.jpg" alt=""></p><a id="more"></a><p>前面文章我们已经知道 Flink 是什么东西了，安装好 Flink 后，我们再来看下安装路径下的配置文件吧。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fwo1nq6751j31go0hewll.jpg" alt=""></p><p>安装目录下主要有 flink-conf.yaml 配置、日志的配置文件、zk 配置、Flink SQL Client 配置。</p><h2 id="flink-conf-yaml"><a href="#flink-conf-yaml" class="headerlink" title="flink-conf.yaml"></a>flink-conf.yaml</h2><h3 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># jobManager 的IP地址</span><br><span class="line">jobmanager.rpc.address: localhost</span><br><span class="line"></span><br><span class="line"># JobManager 的端口号</span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line"></span><br><span class="line"># JobManager JVM heap 内存大小</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># TaskManager JVM heap 内存大小</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line"></span><br><span class="line"># 每个 TaskManager 提供的任务 slots 数量大小</span><br><span class="line"></span><br><span class="line">taskmanager.numberOfTaskSlots: 1</span><br><span class="line"></span><br><span class="line"># 程序默认并行计算的个数</span><br><span class="line">parallelism.default: 1</span><br><span class="line"></span><br><span class="line"># 文件系统来源</span><br><span class="line"># fs.default-scheme</span><br></pre></td></tr></table></figure><h3 id="高可用性配置"><a href="#高可用性配置" class="headerlink" title="高可用性配置"></a>高可用性配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.</span><br><span class="line"># high-availability: zookeeper</span><br><span class="line"></span><br><span class="line"># 文件系统路径，让 Flink 在高可用性设置中持久保存元数据</span><br><span class="line"># high-availability.storageDir: hdfs:///flink/ha/</span><br><span class="line"></span><br><span class="line"># zookeeper 集群中仲裁者的机器 ip 和 port 端口号</span><br><span class="line"># high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line"></span><br><span class="line"># 默认是 open，如果 zookeeper security 启用了该值会更改成 creator</span><br><span class="line"># high-availability.zookeeper.client.acl: open</span><br></pre></td></tr></table></figure><h3 id="容错和检查点-配置"><a href="#容错和检查点-配置" class="headerlink" title="容错和检查点 配置"></a>容错和检查点 配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 用于存储和检查点状态</span><br><span class="line"># state.backend: filesystem</span><br><span class="line"></span><br><span class="line"># 存储检查点的数据文件和元数据的默认目录</span><br><span class="line"># state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># savepoints 的默认目标目录(可选)</span><br><span class="line"># state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints</span><br><span class="line"></span><br><span class="line"># 用于启用/禁用增量 checkpoints 的标志</span><br><span class="line"># state.backend.incremental: false</span><br></pre></td></tr></table></figure><h3 id="web-前端配置"><a href="#web-前端配置" class="headerlink" title="web 前端配置"></a>web 前端配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 基于 Web 的运行时监视器侦听的地址.</span><br><span class="line">#jobmanager.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line">#  Web 的运行时监视器端口</span><br><span class="line">rest.port: 8081</span><br><span class="line"></span><br><span class="line"># 是否从基于 Web 的 jobmanager 启用作业提交</span><br><span class="line"># jobmanager.web.submit.enable: false</span><br></pre></td></tr></table></figure><h3 id="高级配置"><a href="#高级配置" class="headerlink" title="高级配置"></a>高级配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># io.tmp.dirs: /tmp</span><br><span class="line"></span><br><span class="line"># 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存</span><br><span class="line"># taskmanager.memory.preallocate: false</span><br><span class="line"></span><br><span class="line"># 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类</span><br><span class="line"># classloader.resolve-order: child-first</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数</span><br><span class="line"></span><br><span class="line"># taskmanager.network.memory.fraction: 0.1</span><br><span class="line"># taskmanager.network.memory.min: 67108864</span><br><span class="line"># taskmanager.network.memory.max: 1073741824</span><br></pre></td></tr></table></figure><h3 id="Flink-集群安全配置"><a href="#Flink-集群安全配置" class="headerlink" title="Flink 集群安全配置"></a>Flink 集群安全配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 指示是否从 Kerberos ticket 缓存中读取</span><br><span class="line"># security.kerberos.login.use-ticket-cache: true</span><br><span class="line"></span><br><span class="line"># 包含用户凭据的 Kerberos 密钥表文件的绝对路径</span><br><span class="line"># security.kerberos.login.keytab: /path/to/kerberos/keytab</span><br><span class="line"></span><br><span class="line"># 与 keytab 关联的 Kerberos 主体名称</span><br><span class="line"># security.kerberos.login.principal: flink-user</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）</span><br><span class="line"># security.kerberos.login.contexts: Client,KafkaClient</span><br></pre></td></tr></table></figure><h3 id="Zookeeper-安全配置"><a href="#Zookeeper-安全配置" class="headerlink" title="Zookeeper 安全配置"></a>Zookeeper 安全配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 覆盖以下配置以提供自定义 ZK 服务名称</span><br><span class="line"># zookeeper.sasl.service-name: zookeeper</span><br><span class="line"></span><br><span class="line"># 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）</span><br><span class="line"># zookeeper.sasl.login-context-name: Client</span><br></pre></td></tr></table></figure><h3 id="HistoryServer"><a href="#HistoryServer" class="headerlink" title="HistoryServer"></a>HistoryServer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer</span><br><span class="line"></span><br><span class="line"># 将已完成的作业上传到的目录</span><br><span class="line"># jobmanager.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的地址</span><br><span class="line"># historyserver.web.address: 0.0.0.0</span><br><span class="line"></span><br><span class="line"># 基于 Web 的 HistoryServer 的端口号</span><br><span class="line"># historyserver.web.port: 8082</span><br><span class="line"></span><br><span class="line"># 以逗号分隔的目录列表，用于监视已完成的作业</span><br><span class="line"># historyserver.archive.fs.dir: hdfs:///completed-jobs/</span><br><span class="line"></span><br><span class="line"># 刷新受监控目录的时间间隔（以毫秒为单位）</span><br><span class="line"># historyserver.archive.fs.refresh-interval: 10000</span><br></pre></td></tr></table></figure><p>查看下另外两个配置 slaves / master</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fwo7pbjdsgj31jq08i76l.jpg" alt=""></p><h2 id="2、slaves"><a href="#2、slaves" class="headerlink" title="2、slaves"></a>2、slaves</h2><p>里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost</span><br></pre></td></tr></table></figure><h2 id="3、masters"><a href="#3、masters" class="headerlink" title="3、masters"></a>3、masters</h2><p>host:port </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br></pre></td></tr></table></figure><h2 id="4、zoo-cfg"><a href="#4、zoo-cfg" class="headerlink" title="4、zoo.cfg"></a>4、zoo.cfg</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 每个 tick 的毫秒数</span><br><span class="line">tickTime=2000</span><br><span class="line"></span><br><span class="line"># 初始同步阶段可以采用的 tick 数</span><br><span class="line">initLimit=10</span><br><span class="line"></span><br><span class="line"># 在发送请求和获取确认之间可以传递的 tick 数</span><br><span class="line">syncLimit=5</span><br><span class="line"></span><br><span class="line"># 存储快照的目录</span><br><span class="line"># dataDir=/tmp/zookeeper</span><br><span class="line"></span><br><span class="line"># 客户端将连接的端口</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line"># ZooKeeper quorum peers</span><br><span class="line">server.1=localhost:2888:3888</span><br><span class="line"># server.2=host:peer-port:leader-port</span><br></pre></td></tr></table></figure><h2 id="5、日志配置"><a href="#5、日志配置" class="headerlink" title="5、日志配置"></a>5、日志配置</h2><p>Flink 在不同平台下运行的日志文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">log4j-cli.properties</span><br><span class="line">log4j-console.properties</span><br><span class="line">log4j-yarn-session.properties</span><br><span class="line">log4j.properties</span><br><span class="line">logback-console.xml</span><br><span class="line">logback-yarn.xml</span><br><span class="line">logback.xml</span><br></pre></td></tr></table></figure><h2 id="sql-client-defaults-yaml"><a href="#sql-client-defaults-yaml" class="headerlink" title="sql-client-defaults.yaml"></a>sql-client-defaults.yaml</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">execution:</span></span><br><span class="line">  <span class="comment"># 'batch' or 'streaming' execution</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">streaming</span></span><br><span class="line">  <span class="comment"># allow 'event-time' or only 'processing-time' in sources</span></span><br><span class="line"><span class="attr">  time-characteristic:</span> <span class="string">event-time</span></span><br><span class="line">  <span class="comment"># interval in ms for emitting periodic watermarks</span></span><br><span class="line"><span class="attr">  periodic-watermarks-interval:</span> <span class="number">200</span></span><br><span class="line">  <span class="comment"># 'changelog' or 'table' presentation of results</span></span><br><span class="line"><span class="attr">  result-mode:</span> <span class="string">changelog</span></span><br><span class="line">  <span class="comment"># parallelism of the program</span></span><br><span class="line"><span class="attr">  parallelism:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># maximum parallelism</span></span><br><span class="line"><span class="attr">  max-parallelism:</span> <span class="number">128</span></span><br><span class="line">  <span class="comment"># minimum idle state retention in ms</span></span><br><span class="line"><span class="attr">  min-idle-state-retention:</span> <span class="number">0</span></span><br><span class="line">  <span class="comment"># maximum idle state retention in ms</span></span><br><span class="line"><span class="attr">  max-idle-state-retention:</span> <span class="number">0</span></span><br><span class="line">  </span><br><span class="line"><span class="attr">deployment:</span></span><br><span class="line">  <span class="comment"># general cluster communication timeout in ms</span></span><br><span class="line"><span class="attr">  response-timeout:</span> <span class="number">5000</span></span><br><span class="line">  <span class="comment"># (optional) address from cluster to gateway</span></span><br><span class="line"><span class="attr">  gateway-address:</span> <span class="string">""</span></span><br><span class="line">  <span class="comment"># (optional) port from cluster to gateway</span></span><br><span class="line"><span class="attr">  gateway-port:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>Flink sql client ：你可以从官网这里了解 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html">https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文拿安装目录文件下的配置文件讲解了下 Flink 目录下的所有配置。</p><p>你也可以通过官网这里学习更多：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html">https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>本篇文章地址是：<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">http://www.54tianzhisheng.cn/2018/10/27/flink-config/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1fwo0tys9m5j31hc0zfncl.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Apache Flink 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/"/>
    <id>http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/</id>
    <published>2018-10-12T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fww98oig4wj31hc0zbaha.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 是一种流式计算框架，为什么我会接触到 Flink 呢？因为我目前在负责的是监控平台的告警部分，负责采集到的监控数据会直接往 kafka 里塞，然后告警这边需要从 kafka topic 里面实时读取到监控数据，并将读取到的监控数据做一些 聚合/转换/计算 等操作，然后将计算后的结果与告警规则的阈值进行比较，然后做出相应的告警措施（钉钉群、邮件、短信、电话等）。画了个简单的图如下：</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw6mg57rkmj3192170ad6.jpg" alt="监控告警"></p><p>目前告警这块的架构是这样的结构，刚进公司那会的时候，架构是所有的监控数据直接存在 ElasticSearch 中，然后我们告警是去 ElasticSearch 中搜索我们监控指标需要的数据，幸好 ElasticSearch 的搜索能力够强大。但是你有没有发现一个问题，就是所有的监控数据从采集、采集后的数据做一些 计算/转换/聚合、再通过 Kafka 消息队列、再存进 ElasticSearch 中，再而去 ElasticSearch 中查找我们的监控数据，然后做出告警策略。整个流程对监控来说看起来很按照常理，但是对于告警来说，如果中间某个环节出了问题，比如 Kafka 消息队列延迟、监控数据存到 ElasticSearch 中写入时间较长、你的查询姿势写的不对等原因，这都将导致告警从 ElasticSearch 查到的数据是有延迟的。也许是 30 秒、一分钟、或者更长，这样对于告警来说这无疑将导致告警的消息没有任何的意义。</p><p>为什么这么说呢？为什么需要监控告警平台呢？无非就是希望我们能够尽早的发现问题，把问题给告警出来，这样开发和运维人员才能够及时的处理解决好线上的问题，以免给公司造成巨大的损失。</p><p>更何况现在还有更多的公司在做那种提前预警呢！这种又该如何做呢？需要用大数据和机器学习的技术去分析周期性的历史数据，然后根据这些数据可以整理出来某些监控指标的一些周期性（一天/七天/一月/一季度/一年）走势图，这样就大概可以绘图出来。然后根据这个走势图，可以将当前时间点的监控指标的数据使用量和走势图进行对比，在快要达到我们告警规则的阈值时，这时就可以提前告一个预警出来，让运维提前知道预警，然后提前查找问题，这样就能够提早发现问题所在，避免损失，将损失降到最小！当然，这种也是我打算做的，应该可以学到不少东西的。</p><p>于是乎，我现在就在接触流式计算框架 Flink，类似的还有常用的 Spark 等。</p><p>自己也接触了 Flink 一段时间了，这块中文资料目前书籍是只有一本很薄的，英文书籍也是三本不超过。</p><p>我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以关注我的公众号：<strong>zhisheng</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p>另外这里也推荐一些博客可以看看：</p><p>1、官网：<a href="">https://flink.apache.org/</a></p><p>2、GitHub: <a href="">https://github.com/apache/flink</a></p><p>3、<a href="">https://blog.csdn.net/column/details/apacheflink.html</a></p><p>4、<a href="">https://blog.csdn.net/lmalds/article/category/6263085</a></p><p>5、<a href="">http://wuchong.me/</a></p><p>6、<a href="">https://blog.csdn.net/liguohuabigdata/article/category/7279020</a></p><p>下面的介绍可能也有不少参考以上所有的资料，感谢他们！在介绍 Flink 前，我们先看看 <strong>数据集类型</strong> 和 <strong>数据运算模型</strong> 的种类。</p><h3 id="数据集类型有哪些呢："><a href="#数据集类型有哪些呢：" class="headerlink" title="数据集类型有哪些呢："></a>数据集类型有哪些呢：</h3><ul><li>无穷数据集：无穷的持续集成的数据集合</li><li>有界数据集：有限不会改变的数据集合</li></ul><p>那么那些常见的无穷数据集有哪些呢？</p><ul><li>用户与客户端的实时交互数据</li><li>应用实时产生的日志</li><li>金融市场的实时交易记录</li><li>…</li></ul><p>数据运算模型有哪些呢：</p><ul><li>流式：只要数据一直在产生，计算就持续地进行</li><li>批处理：在预先定义的时间内运行计算，当完成时释放计算机资源</li></ul><p>Flink 它可以处理有界的数据集、也可以处理无界的数据集、它可以流式的处理数据、也可以批量的处理数据。</p><h3 id="Flink-是什么-？"><a href="#Flink-是什么-？" class="headerlink" title="Flink 是什么 ？"></a>Flink 是什么 ？</h3><p> <img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw6nryof95j31kw0uv49p.jpg" alt="flink-01"></p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fw6nty7u10j31kw0untsh.jpg" alt="flink-02"></p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fw6nu5yishj31kw0w04cm.jpg" alt="flink-03"></p><p>上面三张图转自 云邪 成都站 《Flink 技术介绍与未来展望》，侵删。</p><h3 id="从下至上，Flink-整体结构"><a href="#从下至上，Flink-整体结构" class="headerlink" title="从下至上，Flink 整体结构"></a>从下至上，Flink 整体结构</h3><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fw7kzdn54tj30lp0cu0td.jpg" alt="flink-stack-frontpage"></p><p>从下至上：</p><p>1、部署：Flink 支持本地运行、能在独立集群或者在被 YARN 或 Mesos 管理的集群上运行， 也能部署在云上。</p><p>2、运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。</p><p>3、API：DataStream、DataSet、Table、SQL API。</p><p>4、扩展库：Flink 还包括用于复杂事件处理，机器学习，图形处理和 Apache Storm 兼容性的专用代码库。</p><h3 id="Flink-数据流编程模型"><a href="#Flink-数据流编程模型" class="headerlink" title="Flink 数据流编程模型"></a>Flink 数据流编程模型</h3><h4 id="抽象级别"><a href="#抽象级别" class="headerlink" title="抽象级别"></a>抽象级别</h4><p>Flink 提供了不同的抽象级别以开发流式或批处理应用。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fw7i1n19qpj318i0iwwg1.jpg" alt="2018-10-14_09-34-17"> </p><ul><li>最底层提供了有状态流。它将通过 过程函数（Process Function）嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。</li><li>DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换 / 计算。</li><li><strong>Table API</strong> 是以 <em>表</em> 为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。</li></ul><p>你可以在表与 <em>DataStream</em>/<em>DataSet</em> 之间无缝切换，也允许程序将 <em>Table API</em> 与 <em>DataStream</em> 以及 <em>DataSet</em> 混合使用。</p><ul><li>Flink 提供的最高层级的抽象是 <strong>SQL</strong> 。这一层抽象在语法与表达能力上与 <em>Table API</em> 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。</li></ul><h4 id="Flink-程序与数据流结构"><a href="#Flink-程序与数据流结构" class="headerlink" title="Flink 程序与数据流结构"></a>Flink 程序与数据流结构</h4><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw7ij4194jj31a00usq7o.jpg" alt="2018-10-14_09-51-09"></p><p>Flink 应用程序结构就是如上图所示：</p><p>1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。</p><p>2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。</p><p>3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。</p><h3 id="为什么选择-Flink？"><a href="#为什么选择-Flink？" class="headerlink" title="为什么选择 Flink？"></a>为什么选择 Flink？</h3><p>Flink 是一个开源的分布式流式处理框架：</p><p>①提供准确的结果，甚至在出现无序或者延迟加载的数据的情况下。</p><p>②它是状态化的容错的，同时在维护一次完整的的应用状态时，能无缝修复错误。</p><p>③大规模运行，在上千个节点运行时有很好的吞吐量和低延迟。</p><p>更早的时候，我们讨论了数据集类型（有界 vs 无穷）和运算模型（批处理 vs 流式）的匹配。Flink 的流式计算模型启用了很多功能特性，如状态管理，处理无序数据，灵活的视窗，这些功能对于得出无穷数据集的精确结果是很重要的。</p><ul><li>Flink 保证状态化计算强一致性。”状态化“意味着应用可以维护随着时间推移已经产生的数据聚合或者，并且 Filnk 的检查点机制在一次失败的事件中一个应用状态的强一致性。</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw7l8wbujfj30at05d74b.jpg" alt="exactly_once_state"></p><ul><li>Flink 支持流式计算和带有事件时间语义的视窗。事件时间机制使得那些事件无序到达甚至延迟到达的数据流能够计算出精确的结果。</li></ul><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fw7l9248cwj308c024wei.jpg" alt="out_of_order_stream"></p><ul><li>除了提供数据驱动的视窗外，Flink 还支持基于时间，计数，session 等的灵活视窗。视窗能够用灵活的触发条件定制化从而达到对复杂的流传输模式的支持。Flink 的视窗使得模拟真实的创建数据的环境成为可能。</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw7l97x9lkj30ok06ca9x.jpg" alt="windows"></p><ul><li>Flink 的容错能力是轻量级的，允许系统保持高并发，同时在相同时间内提供强一致性保证。Flink 以零数据丢失的方式从故障中恢复，但没有考虑可靠性和延迟之间的折衷。</li></ul><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw7l9et8bxj30cf0elt9t.jpg" alt="distributed_snapshots"></p><ul><li>Flink 能满足高并发和低延迟（计算大量数据很快）。下图显示了 Apache Flink 与 Apache Storm 在完成流数据清洗的分布式任务的性能对比。</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw7l9l2tp7j30t80af3zq.jpg" alt="streaming_performance"></p><ul><li>Flink 保存点提供了一个状态化的版本机制，使得能以无丢失状态和最短停机时间的方式更新应用或者回退历史数据。</li></ul><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw7l9qnao9j30gj0a73zq.jpg" alt="savepoints"></p><ul><li>Flink 被设计成能用上千个点在大规模集群上运行。除了支持独立集群部署外，Flink 还支持 YARN 和Mesos 方式部署。</li><li>Flink 的程序内在是并行和分布式的，数据流可以被分区成 <strong>stream partitions</strong>，operators 被划分为operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为1；</li></ul><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fw7l9vt5urj30jb0crtag.jpg" alt="parallel_dataflows"></p><ul><li><p>自己的内存管理</p><p>Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。</p></li><li><p>丰富的库</p><p>Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等。 由于其架构，很容易执行复杂的事件处理和警报。 </p></li></ul><h3 id="分布式运行"><a href="#分布式运行" class="headerlink" title="分布式运行"></a>分布式运行</h3><p>flink 作业提交架构流程可见下图：</p><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fw7ovrkditj30op0fpjsp.jpg" alt=""></p><p>1、Program Code：我们编写的 Flink 应用程序代码</p><p>2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fw96gl5l9zj30pp089dh7.jpg" alt=""></p><p>3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件</p><p>4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwly1fw96irwzj5j30pj0c3dir.jpg" alt=""></p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本文主要讲了我接触到 Flink 的缘由，然后从数据集类型和数据运算模型开始讲起，接着介绍了下 Flink 是什么、Flink 的整体架构、提供的 API、Flink 的优点所在以及 Flink 的分布式作业运行的方式。水文一篇，希望你能够对 Flink 稍微有一点概念了。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNbRwly1fww98oig4wj31hc0zbaha.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</title>
    <link href="http://www.54tianzhisheng.cn/2018/09/18/flink-install/"/>
    <id>http://www.54tianzhisheng.cn/2018/09/18/flink-install/</id>
    <published>2018-09-17T16:00:00.000Z</published>
    <updated>2019-03-17T13:42:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fvsli4dkgzj31hc0zfass.jpg" alt="009"></p><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>1、安装查看 Java 的版本号，推荐使用 Java 8。</p><h3 id="安装-Flink"><a href="#安装-Flink" class="headerlink" title="安装 Flink"></a>安装 Flink</h3><p>2、在 Mac OS X 上安装 Flink 是非常方便的。推荐通过 homebrew 来安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install apache-flink</span><br></pre></td></tr></table></figure><p>3、检查安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink --version</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Version: 1.6.0, Commit ID: ff472b4</span><br></pre></td></tr></table></figure><p>4、启动 flink </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zhisheng@zhisheng  /usr/local/Cellar/apache-flink/1.6.0/libexec/bin  ./start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host zhisheng.</span><br><span class="line">Starting taskexecutor daemon on host zhisheng.</span><br></pre></td></tr></table></figure><p>接着就可以进入 web 页面(<a href="http://localhost:8081/">http://localhost:8081/</a>) 查看</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvdtbrqnqcj31ao0iamz8.jpg" alt="1flink-web"></p><h3 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h3><p>1、新建一个 maven 项目</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fvdtdwzs0lj30sf0j7ae9.jpg" alt="flink-demo"></p><p>创建一个 SocketTextStreamWordCount 文件，加入以下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhisheng_tian on 2018/9/18</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketTextStreamWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//参数检查</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">"USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;"</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        String hostname = args[<span class="number">0</span>];</span><br><span class="line">        Integer port = Integer.parseInt(args[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// set up the streaming execution environment</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计数</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(<span class="keyword">new</span> LineSplitter())</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        sum.print();</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"Java WordCount from SocketTextStream Example"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">LineSplitter</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt; </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> </span>&#123;</span><br><span class="line">            String[] tokens = s.toLowerCase().split(<span class="string">"\\W+"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String token: tokens) &#123;</span><br><span class="line">                <span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(token, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接着进入工程目录，使用以下命令打包。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fvdth7y4k1j31b40gxwi9.jpg" alt="2build"></p><p>然后我们开启监听 9000 端口: </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -l 9000</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvdtiq9l78j30nd04h3yp.jpg" alt="监听"></p><p>最后进入 flink 安装目录 bin 下执行以下命令跑程序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -c com.zhisheng.flink.SocketTextStreamWordCount /Users/zhisheng/IdeaProjects/flink/word-count/target/original-word-count-1.0-SNAPSHOT.jar 127.0.0.1 9000</span><br></pre></td></tr></table></figure><p>注意换成你自己项目的路径。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvdtl0aml1j31kw05hgn9.jpg" alt="4run"></p><p>执行完上述命令后，我们可以在 webUI 中看到正在运行的程序：</p><p> <img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fvdtmqzvc8j31a10d7q41.jpg" alt="5running-job"></p><p>我们可以在 nc 监听端口中输入 text，比如：</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fvdtrr7bvhj30mn08lt97.jpg" alt="nc"></p><p>然后我们通过 tail 命令看一下输出的 log 文件，来观察统计结果。进入目录 apache-flink/1.6.0/libexec/log，执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail -f flink-zhisheng-taskexecutor-0-zhisheng.out</span><br></pre></td></tr></table></figure><p> 注意：切换成你自己的路径和查看自己的目录。</p><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvdtw41tyoj31kw0bf0vd.jpg" alt="result"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文描述了如何在 Mac 电脑上安装 Flink，及运行它。接着通过一个简单的 Flink 程序来介绍如何构建及运行Flink 程序。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请注明地址：<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">http://www.54tianzhisheng.cn/2018/09/18/flink-install</a> </p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fvsli4dkgzj31hc0zfass.jpg&quot; alt=&quot;009&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Go 并发——实现协程同步的几种方式</title>
    <link href="http://www.54tianzhisheng.cn/2018/08/30/go-sync/"/>
    <id>http://www.54tianzhisheng.cn/2018/08/30/go-sync/</id>
    <published>2018-08-29T16:00:00.000Z</published>
    <updated>2018-11-22T14:51:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fus2khvi22j31hc0zfahk.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Java 中有一系列的线程同步的方法，go 里面有 goroutine（协程），先看下下面的代码执行的结果是什么呢？</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 1"</span>)</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 2"</span>)</span><br><span class="line">    &#125;()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行以上代码很可能看不到输出。</p><p>因为有可能这两个协程还没得到执行，主协程就已经结束了，而主协程结束时会结束所有其他协程，所以导致代码运行的结果什么都没有。</p><p>估计不少新接触 go 的童鞋都会对此郁闷😒，可能会问那么该如何等待主协程中创建的协程执行完毕之后再结束主协程呢？</p><p>下面说几种可以解决的方法：</p><h3 id="Sleep-一段时间"><a href="#Sleep-一段时间" class="headerlink" title="Sleep 一段时间"></a>Sleep 一段时间</h3><p>在 main 方法退出之前 sleep 一段时间就<strong>可能</strong>会出现结果了，如下代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 1"</span>)</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 2"</span>)</span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    time.Sleep(time.Second * <span class="number">1</span>) <span class="comment">// 睡眠1秒，等待上面两个协程结束</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这两个简单的协程执行消耗的时间很短的，所以你会发现现在就有结果出现了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Goroutine 1</span><br><span class="line">Goroutine 2</span><br></pre></td></tr></table></figure><p>为什么上面我要说 “可能会出现” ？</p><p>因为 sleep 这个时间目前是设置的 1s，如果我这两个协程里面执行了很复杂的逻辑操作（时间大于 1s），那么就会发现依旧也是无结果打印出来的。</p><p>那么就可以发现这种方式得到问题所在了：<strong>我们无法确定需要睡眠多久</strong></p><p>上面那种方式有问题，go 里面其实也可以用管道来实现同步的。</p><h3 id="管道实现同步"><a href="#管道实现同步" class="headerlink" title="管道实现同步"></a>管道实现同步</h3><p>那么用管道怎么实现同步呢？show code：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span><br><span class="line">    count := <span class="number">2</span> <span class="comment">// count 表示活动的协程个数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 1"</span>)</span><br><span class="line">        ch &lt;- <span class="keyword">struct</span>&#123;&#125;&#123;&#125; <span class="comment">// 协程结束，发出信号</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 2"</span>)</span><br><span class="line">        ch &lt;- <span class="keyword">struct</span>&#123;&#125;&#123;&#125; <span class="comment">// 协程结束，发出信号</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">range</span> ch &#123;</span><br><span class="line">        <span class="comment">// 每次从ch中接收数据，表明一个活动的协程结束</span></span><br><span class="line">        count--</span><br><span class="line">        <span class="comment">// 当所有活动的协程都结束时，关闭管道</span></span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span> &#123;</span><br><span class="line">            <span class="built_in">close</span>(ch)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种方式是一种比较完美的解决方案， goroutine / channel 它们也是在 go 里面经常搭配在一起的一对。</p><h3 id="sync-WaitGroup"><a href="#sync-WaitGroup" class="headerlink" title="sync.WaitGroup"></a>sync.WaitGroup</h3><p>其实 go 里面也提供了更简单的方式 —— 使用 sync.WaitGroup。</p><p>WaitGroup 顾名思义，就是用来等待一组操作完成的。WaitGroup 内部实现了一个计数器，用来记录未完成的操作个数，它提供了三个方法：</p><ul><li>Add() 用来添加计数</li><li>Done() 用来在操作结束时调用，使计数减一</li><li>Wait() 用来等待所有的操作结束，即计数变为 0，该函数会在计数不为 0 时等待，在计数为 0 时立即返回</li></ul><p>继续 show code：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"sync"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line">    wg.Add(<span class="number">2</span>) <span class="comment">// 因为有两个动作，所以增加2个计数</span></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 1"</span>)</span><br><span class="line">        wg.Done() <span class="comment">// 操作完成，减少一个计数</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">"Goroutine 2"</span>)</span><br><span class="line">        wg.Done() <span class="comment">// 操作完成，减少一个计数</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    wg.Wait() <span class="comment">// 等待，直到计数为0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>你会发现也是可以看到运行结果的，是不是发现这种方式是很简单的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>多看别人写的代码；多想想为啥要这样写；多查自己不理解的地方；多写 demo 测试；多写文章总结。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p><img src="http://image.54tianzhisheng.cn/blog/180103/C6LG3mGa12.jpg" alt="zhisheng"></p><p>本文地址为：<a href="http://www.54tianzhisheng.cn/2018/08/30/go-sync/">http://www.54tianzhisheng.cn/2018/08/30/go-sync/</a> ，转载请注明原文出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNbRwly1fus2khvi22j31hc0zfahk.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="GO" scheme="http://www.54tianzhisheng.cn/tags/GO/"/>
    
  </entry>
  
</feed>
